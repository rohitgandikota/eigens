{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "285aab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar, TraceDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43af3fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): ReLU(inplace=True)\n",
       "    (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "__all__ = [\n",
    "    \"VGG\",\n",
    "    \"vgg11_bn\",\n",
    "    \"vgg13_bn\",\n",
    "    \"vgg16_bn\",\n",
    "    \"vgg19_bn\",\n",
    "]\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=10, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        # CIFAR 10 (7, 7) to (1, 1)\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),\n",
    "            # nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"D\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "    \"E\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, device, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs[\"init_weights\"] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(\n",
    "             \"state_dicts/\" + arch + \".pt\", map_location=device\n",
    "        )\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg11_bn\", \"A\", True, pretrained, progress, device, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg13_bn\", \"B\", True, pretrained, progress, device, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg16_bn\", \"D\", True, pretrained, progress, device, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg19_bn\", \"E\", True, pretrained, progress, device, **kwargs)\n",
    "\n",
    "model = vgg13_bn(pretrained=True)\n",
    "\n",
    "model.to(device)\n",
    "# out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7636be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2b1a15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAACwCAYAAACviAzDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABLg0lEQVR4nO2deXQc1Znov1p609KtxZZkIckWscEY2yzeEBBCwBMgGQLBkxAOE0zCmQwTOwH83gScDMyZzDDmTd4bSOY45EwOA5kTHAh5LAmZwIAhEIh3MOAYbIN3rNVyd0u9V9V9f/BS9/u+ttqSLVqW9f3O0Tn39ldddevWrdtX99sMpZQCQRAEQRCEMmGOdQMEQRAEQZhYyOJDEARBEISyIosPQRAEQRDKiiw+BEEQBEEoK7L4EARBEAShrMjiQxAEQRCEsiKLD0EQBEEQyoosPgRBEARBKCuy+BAEQRAEoazI4kMQBEEQhLLysS0+Vq9eDdOmTYNwOAyLFi2CjRs3flyXEgRBEARhHGF8HLldHn/8cbjpppvgxz/+MSxatAgeeOABeOKJJ2DHjh3Q0NBQ8rue58GhQ4eguroaDMMY7aYJgiAIgvAxoJSCgYEBaG5uBtM8xt6G+hhYuHChWrZsmV93XVc1NzerVatWHfO7Bw4cUAAgf/Inf/Inf/Inf+Pw78CBA8f8rbdhlMnn87BlyxZYuXKl/5lpmrB48WJYt25d0fG5XA5yuZxfV/9/I+aOO+6AUCg02s0TBEEQBOFjIJfLwf333w/V1dXHPHbUFx99fX3gui40NjaSzxsbG+G9994rOn7VqlXwD//wD0Wfh0IhWXwIgiAIwjhjOCYTY+7tsnLlSkgkEv7fgQMHxrpJgiAIgiB8jIz6zsekSZPAsizo7u4mn3d3d0NTU1PR8bLDIQiCIAgTi1Hf+QgGgzBv3jxYu3at/5nnebB27Vro6OgY7csJgiAIgjDOGPWdDwCAFStWwNKlS2H+/PmwcOFCeOCBByCVSsFXv/rVEz73rMu+QuoGeLpsUj0T1zuZuG4MfSw7DRgwfJdfhS/BvkdWelwnpmgdO0ArUETmkeOozKBV8l0motcoJWTn8TwmK3Ei3naFvrvj1TUwFA0b9pG65xboedCN2rbFrqF7KF9wiMwOBEk9EAr7ZZd3gqe/a1tsvLBjHcf1y6YZIDIzoOsFg96HbelREbJo2wrZPK3ncJ1e30J94LoekXmuS+oGcoHj74jj0PaRa1h0usD9nMlk2cH6GoOXzBrynAAA/+fHP9TnnDSJyGa3t/nlC8NHiCwKtH+uX/FP+jwheh/ZDzf55YEBuivrVc8m9eYpF/vlF1/4CZE98MCjfnl/H71+KKyfc0uMjslzm+mYmFKj5ekB+rxSOf1suxL0Pg7tp+O5YbLuZ9emz6fxNH2NqdNjRNY6/TxSP2POpX45XD2VyGoadb2ykhoS8mnsZz8b+p3uq6j1y6bbT2SOl6YHu3EkHKQiJ4nKOSLzaPeAUrr/LPoIIAD6Awfo88qy99vL62dUyNHnBYZ+by0zQkVKX8Mt0O/lcvTZ5vJJVKZjvYDeac/lvyv0uYfCUb8cCFfRtqJOUAa9Z0fxeVSPb1vRZ9Ax/UI4UT6Wxcf1118Pvb29cM8990BXVxece+658NxzzxUZoQqCIAiCMPH4WBYfAADLly+H5cuXf1ynFwRBEARhnDLm3i6CIAiCIEwsPradj48LkxtkoPUTl3F9Nq4X+yEPbfNRCn6ekjYfIwgXX9oeA19v+OfkdgqlrsFtSfABpsn0odgGpMjohF2Ty4fCose57GsG0U8G2LHaxkExPXiBKX69oNbROibVeZqgFcimospks6DYsciuIsieSaW+ZkUkSkQm0uW6gxki42PLRrYanqL6Y9PQsqI+ZmGOqX0TG7+oDyyL9gdvD25B0bt3rNDKiCAaTypPdfgVqG4AvefeBLXd+GC3tuuITqY2DlETPT+TPstCgdobvL19q1/uZHYdedA2QgWP2tI46PF15eizbJlBqrCoXZcHE7Q9eXTaNzrpM3hrH7UTwDYhAUXb05nWz2TbIdqvl9spUp91vr4vk9k34XFwrDm2FB6yaQBm46EU7WdA9+Kxl991DFSm40xxeyc0//CZB5s8GMz+wfboeXP4y6wPTFP3nVLUbstF53EdNm+6dG5yXd0HrqK2I2Dq52x4zP7No/ZWeE4xstRWA9t5cJsPD7gNCLKXsdnzGQVk50MQBEEQhLIiiw9BEARBEMrKuFO7ANtuxptpqshddfhqF1znVyjer9NFk6lW1FAHAoCnht6i5C6pRdfEIm/ow4ouQVQrQ5+0yMu0hDstP49CDSqWla4PDT0uy56Xi1xmTZtudebQNm2eq2vCdDvTQe6jyTzdzqyrqtRlm21F59jWK7qOxb3bovoaio2J1OG4rrDr07s6htv0EOWj1XELit4m9Hw8j0q5Ggb7WRY91REky1bovt1EksiMjFYXmMwVORysI/XKCl2vCNcQmelo10X2KOFwvI/Uv3//w375g32dRFZXpz32QiGmLkGulK7Btt+zTB2Q1mqZCtYerLUzFVUTDrIXPI0GuMVUBf053b7CETq2FqTpsaap28tVZhZSxZ1IpvFCrscv24q51rJ/g5WB5xQq81ykYvSYeyifbzykomG/HR5SXwRM+lNoMPWJUsg93QrTY/Gz9uj3HOTmroCqxZTBbtpEwTbZNXCIAsOkz9KEEnWu6kHjh7sXu+whGFgdahT9Kp4wsvMhCIIgCEJZkcWHIAiCIAhlRRYfgiAIgiCUlXFn82GZ3K4DlaG0GxiJrs6WXVhWpCPnes4Sak/sujiSsOz8pOReeAh11HaD6TF5271ht4Fdo8gltpRVwfAZrimAMmmywbRB9aWDtu4Eh7kG5tCa2jHoEK+souGhc8iugTqlAVRVaPuQHLN3CFRSl1kbufCaEWoLkHYTuj0p5vqGQiyHmZ0Ntyfy8Pgt4cbN3a+POZ7xsegBuSwseynX2xOxBciltKtgOkddQI806L7LslDRdbFmUq+fpMOk25XMbXBQZ8s2C6zv8tRV0TJ0fXCQuqguWDTNL0eqqH5/2x/fRyelMs+sJPW80jYfYR72G9kfZFlagaL3mbhfm+xY5CLL7UqCzKYB/x/K0gOMxG26FPmsdo32FO1XI8A6Afcfey88ZNvCQ43z+cVA77/H3HkdR7+nOAw7AEAuw+ydqup10yxqN2YCmqsMllrB0tdwmUusFaT9aqPvFvLMDReFFlDc7tGj9+Vh933WIdiNO8/jF7DxYweQrY81+ksF2fkQBEEQBKGsyOJDEARBEISyIosPQRAEQRDKyriz+SgKZ17SiKBUhIMTaEOpcxIblKHbxu8jm6a67sGk1nXXTW4a6hLFIdP5hUrFCykZu6NUTJDjkx2zQYgEi+mwP3mY1FNID103mWZLro7W+GXez5WVFaReUaH1tcEgfR3CyC/fy1P7hyMOtUlRYW1LYiv6LFM9h/xyJp4gsmoUpt1m4Y4N3lfY1qeE232pGDYjOXb4MVmOZhc1/HftS9ct8cuJDNW9NzfpNOzO3s1ElmPtS2f1mHFcGkeiAunBTRaSO8zaOu8cnUL+YBd9Xvv27PLLl156AZE1oZDub2/ZTtvKwsa7LrI3YPZVlqXbmmM2H/z/RQvp7V1g+eTRIGFmUVARYlM/CR9O370TsefBONkBXTaZPRFru4ViWSgWv8RD9g8eS62gmP2DhSLZKHYsePqaWfZ8ci61DYuAttkxPWbzgW1kbBamHYczt+g9uw5tq4Hu0zLo/OKCPtZhNjDg0OdTQMZhHovkU0CxaAo52h4eWchCtkcGS0sxGsjOhyAIgiAIZUUWH4IgCIIglJVxp3YpCXcPLbEVXHIr+linGaJ8LPA2ts1cy3q7DpD61k1/8Muf+fwNRBaO6C1Aj2XW5GoYvEM3km30UpRSfRVlPvWOLyzvBzm6DdqdpVuUNfXa9S1WRTOYNtZqN9hK5sdYW03VLjGkdnGzVF0ykIj7Ze6Gq5j7YQ5tE7vJOJEN9Pb65XyWbi+H0NamybY2TR78nAy8odUlJmsbd5ktdezxql04I/nuIHI/bq6mLsxT0E7wxgM9RKZa60ndCOjt8Aza4gcAOLhXh1A30lQlc+a8eaR+RVWLX+48RM+z9hWtTkn0UtnNX9Hv6S/sX9C2HXmD1C2sOijQMYHVf0wEBaDvBQ0vwLbRcch/m7lRhuhYc9BQC3KXb3wJxWMUsJD7JfAc/Zw95pKqeGhvJDaghOuvoj9hHnOjttB4Dgbpuz84qN/3Av9egLp1O3l9nzbTYRlh3R4+r5tB1D8WfXZemj0vEk6AhT7AWcWLMo7Tquti92umTkeuySbw0PSsn5HqxzB4socTR3Y+BEEQBEEoK7L4EARBEAShrMjiQxAEQRCEsjLubD6Kwn6jOre/KA5BjSql3EX5eYoONY5a/Kh92K6Eru0KWR1SOZE8QmQHdr9L6u++o90Kp5zWRmT1yLU0GKJuX+EKGsbZQunmwxUs17s5tL6Wu3mWUuFTl11qp8CfyXAtAQ4WaN81Tp5E6tMatQvmlEn0vpqbtC2AZdIrug51XcymB1CZ2nzkUfhlcGl7IszmYhAp51MF2gfhmga/HA0yF7q4Hgcus2uxWGdh/XWhxAM5lmtkKXsMbgNSilL2ISOxFnl3n7Z32pmgLtVQ0M/LY/8rfeKSM0l9V78O371t+1tEdnifdpltnnwakVWrdlLfd3invqZFXS5nz5vhl/sHqUGGY+pxOHnKVCLz+jfSOk4BwGw1nIAOw51nNhbcObJUR+N3mIdIt0NMh48mOYud1FR4PDE7gRFYvSmFbW3ovGUwewM8noqGJLZZYnNsIETnAkvpZ2QF6DWxuzP/KWResGChNPU2nzcNHIacvvs2+tFxPRbyn9nh4Fsx2bwVQJOBfSyvV+RqW2DXVMhexWIda9phUo+EtY2MLeHVBUEQBEEY78jiQxAEQRCEsjIO1S4lVCn8WL4niVUCJYKfFmWjLbGNnc9nSP1In9767Tq4n8gG4v1+ufsQda3dv2cnqR/u1N9d++zjRNbcMs0vN0xpJbIIi+AZq9EqiLomut0crZusvxeh7qpege074g5jbnK4n13mWlvs7Ty8bdoQcwVsm0Lbd1qt3l6tr6VbrREUvTGVpk6y2Qx9Xnh712GerQptNdKtZ4AgX7cjl2c7Qrd365B7byBAv+dEUDbcQ9SVFLI0C6aJrmkYvJ+PLwplKVfbY0VKLeWyOxK1yzf+x//0y/t3UPXEnv36+QWqqOqtehZVbfzi1bV+OZOiCorT27Q7rd1KIwa/1UPVbYC25z/559cQkRHR99nbScfSjv0f+uUzZ8+n37OomtUK6bnAUTSKai6i2x6qoWpCEw6RuiJqh6GjFFvMPTTIIpyaaFvfYlE6ifq4KG3s8F3pTUPPKS5/13hdVaEyuyQp07bW1VL363RS93NigL5PWMvpsgyvBvDIsvpZe0xd4nm6LxWf/9B7YbOozaEQVZErPAE5LBqql9THsQ6xeIp2pKoLsN8y4nXPVHrBEFUJ22iMGPbwXaqHi+x8CIIgCIJQVmTxIQiCIAhCWRnx4uPVV1+Fq6++Gpqbm8EwDHj66aeJXCkF99xzD0yZMgUikQgsXrwYdu3adfSTCYIgCIIw4RixzUcqlYJzzjkHvva1r8F1111XJP+Xf/kX+OEPfwg//elPob29He6++2644oorYPv27RAOh49yxpHCdHNEpXUMF8MSob6xnQfPNmgHQuxgrf/KpKhNwUBC6xizGRp+ORTR52lspvYXNnOtivd2+uW62hoiO/PsuX45Vk/11yZziQqFdJ9zt1ysp+f6/Fya6qE9dGy4gtpfEAdAnuRXlXpeQzM5Qp9Vlc1cz2zs5kmPTQ5o/WguS0Nplwo1ztvmovPmHBbnOkDruZzWCYeZPj0a0/rrUIBl2Z1c45ezTHeb/mAfbSvS0XJ7ppGERR+JXcdwz1MU0n0EYfVbm7UrciHVQGS7+3VfDlg1RHYoSZ9B+0z9XhgsK2lFRNs35UNUf22Z1HajrV23oRLoXBCoQqkN1A4i+2Cndo//3BWXEdmkTy4g9Xintvk6wGzDpkzR1//iOXS87kr8iNTfeHubX+au/XieCrCw36Egy2BqIXfRIkMt5ErPQv7zd68UFjqPZ1D7i6LzKJRFlvva4vdA0WcZsHlmVn3fuSydq3GmVuXSsVQo0GOVp0Oj84jyZhDbRjCXZlPP+ZZJf/8Mm/38BlG/M5uPvKPTA3CDqoBV5IusT8PnYxe/+ywDL/O+tlF7FE+LPAqMePFx1VVXwVVXXXVUmVIKHnjgAfi7v/s7uOaajwy1/vM//xMaGxvh6aefhi9/+csn1lpBEARBEMY9o2rzsWfPHujq6oLFixf7n8ViMVi0aBGsW7fuqN/J5XKQTCbJnyAIgiAIpy6juvjo6uoCAIDGxkbyeWNjoy/jrFq1CmKxmP/X2tp61OMEQRAEQTg1GPM4HytXroQVK1b49WQyWXIBUhTGGYefKLI3YF8mchZ2G+n8+nuoL33NJGpXEUIp7cNhakfR3Hq6X26ZOp3IIii8eXqA+v3veGcTbQ/SmTc1txBZw5RmXT7tdCKritaSuoMc6hWLVeF6+p4VD6nM9KxYJ5xjsTMyKW3bUhFjac6ZHnq4NgUNFbQ9IZPZaiDdczZHdfYe6juX6XL5c8f3mc+z0Os5bS8SYHY/PNqwyurvFlgQg7yLzqvofURrdQr5xk/QZ7nnIF2wmygFtzmC+Aqc4T4Dflzxu6eGJTsWXl4/o0yKPoNERttcdA3Sd6Y/TndJz52rbSUygzR2R3dWfzdUScdkTSXty2BWx2WpYrYRZkp/t6l1BpG1nq7nCYO/h5V0nghN1m0NR+n7bVZoe5XZMRq356//mqZl//6//G+/vOfDTiJzUdwIHrvIYjEnHJRSXnn02Xmefr+Kw+8P///XqrC+5mCG3gefvC30fhent8D2KSwmiclsSRz9Diug77AHaP5TtD3KpbZiHnlt6cvvBvR95ZhthGVqmeL5EtgrTG2oWFwWW7ddGdyuZGi7OtNj7zBqOp8F7KDL6mjuHEHaheEyqmdsavro5evu7iafd3d3+zJOKBSCaDRK/gRBEARBOHUZ1cVHe3s7NDU1wdq1OtJgMpmEDRs2QEdHx2heShAEQRCEccqI1S6Dg4Pw/vvv+/U9e/bA1q1boa6uDtra2uD222+Hf/qnf4IZM2b4rrbNzc1w7bXXjkqDuUsW3nAqysQKPEywludSdFs2NahdS6sqadhbsyiXpD5PgLkPBwG7MrGvoQ/sIN2GbZt+NqlPPUO7DfKtTlwPsOyUnkfVDHj3u1SYYq52CQbp0DBM7KpHr1ER0X2QHYgTmRWg56moroPhUFdFvxdg2SKx2iWfp1utBZQJlW9fclVCAWWgzbJw5oC2UIMRuv3Ngy+TDKIWHXcDaRSaOU+3c7tTWnZ6jLqZBmI1pJ7L6O8WpQDAbTmGWmUkbrnlYDCP3MErphFZ3RStxsuykOnZ7n5SX/Pwr/xyzST6flVG9Xebmmg4ftVEd1vtgL5mZSVVn7hZPV4CWfq9yqoav/zeFrqN3zaZVKEaTTF19dTt3kMqor7DVJVy1iyqyv3m8lv88vbdNGVDwdFzQzV7n2edNZfUo9X6niMR2j/xhA77X8vaqhTt51JUhvUckkpRVanF3UWRetJzaF/ijMCuyzLFFujYx+kUCgU6V7vYjdqj76UJTA2D5lUeir1Q0POGVWDqNuzCDPR3xWPnwT9t3LMVh7xXLK2tbdLZCP/OWdyDGane+SxhsXAGJvoydy8eDUa8+Ni8eTN8+tOf9ut/stdYunQpPPLII/Dtb38bUqkUfP3rX4d4PA4XX3wxPPfcc6MU40MQBEEQhPHOiBcfl156acn/lgzDgO9973vwve9974QaJgiCIAjCqYnkdhEEQRAEoayMuavtiaP1Uh5zKwJFbROOHEYpy1n456oKrbsMBql+3/CYWxhSzingNgXosKINIv1BkLnoTq5o4wejk3J9KApTrIa28QAAMJEbVlEEY1TmXVdsNYDsXJj+GGdbZlmawWNhghN91BNqKEIs9TwP6+wgHSzXnbrommaQ6kcdFiYd23mEeUppbOdh0PO4edaegr5mTtGQ3GHkitd/mLqH7uk/7Jcz02jb5tRT+5hMt07ZfiKurcNlJKHXT6Q9ubw+b92UTxBZxZGdftlLUb38OefRkOWTQtpFtbOfjjMPvQdulr7fAwmqiy+4yI7Kps9Smfr5OS6zC0Bp1+OH6bj/1NxmUp+MMxTUMvsQZK9iWuz9dul7sWDRJX65vuUwkYUj+iKnt0whskqWvgDbRsWPxIks0XvQLzfWTSIysIevTg8F9ERhsvFhsvfb83S/O9z1F1lcuS61eUvE6fNyDT1XFQq0n12l7fwMl9oAGgbtdxewKz29hpfRoQfCBk/foO1nXG6PZ9JxaKFnwCOvY5s8j4VMMItsG9F1DP77MHSKBv47Q1zpR5AuYbjIzocgCIIgCGVFFh+CIAiCIJQVWXwIgiAIglBWxp3NB49Hgasu0yMOHukhdRvFx6isonpekmbcpfEeLKY3A6SPBIvaCRhEd8l15DjoBgt1zuwWzBJpz4n2jaeshxKUEjIZTlkPAODhNrg81orunxCL6+GwOOSWwexnhmpOUVpvqtcsFNAzYfpI+l3mk+9Qn3is6w5y+xD0TLjGM5uj56kIaT19PENjGAxiFWyA6nk9S+uLBwpMPxtjOa5R33oZdh+kf4Zvb3Esz7XjZSQ2Hwaynahtonmh8klt5zLQR20+GufNJvXLzv2MX04maF/m3KHj1BgsfreFxr4doOPOQiGyTYuOF0D2BXzATInRY6NhbBxG3wkLpTIPKxpzg4/1LLqvWA21Iyvkkd0EC1WUd2l7wiHd9s6uvUTmduv6XmY3EaemEiVx0XftorgRbE5BY4LbP2TRedwCPVGCTS/ZHLLFcpjtiKPjqcQi9P/wVI6FVzf0+2Z4tO9aYzqIS/tUmhokVtvul3uSNA7L+529pG4F9LO2WWwpF/cPs4EBbvPm6u8q4DYogGTMxsNlNl6e7ni39C/LcSE7H4IgCIIglBVZfAiCIAiCUFbGndqFg7d3eRjyapakzsRZXJmLqonWYYZHXalsFsLcRG5XeZYpEVN615pulQUC9BrxPr0lV8jTvcRJTToLZvF2GHdhG0oC4KFPTK52YeGOCzh8ODsPvs8cUx1YzH3LDAx3yDF3Mq6FQX7DXD2A+4Sr4ngf2AG9hZrOUHUb3nY0WHj1DDtTfUhv/wY8eo+9aOu3krlYT2/WLpjVbIvfsVn2ykk6/Hph34dEhh8Xd0vmbo1415b/94H70juGe51RQjVYnP20xHlQX0ZYiGdjULuPfrD5JSLb1U6fSVvtxX75jFbqWorVmIqlNgiyWNYh1JkVYbrFblp4G5ttUyvsc87C+PPkyth1nakmnTxS2zG1ghWgrq0hNEba2mh/4Oy0hkvVdNxFP41SADQ0UNVXYmCfX/6/jz1KZBs37SD1K25eDkMRRO++wfz+PaYOdZDqoDJC59gCupd0boDInDwLwV/QfcCfQUONnnNNj15/IEP1SQq5ap8xjbpNf+bCObqtYarO7+nV4fHNQB+R9YdpH8TzOIQDS/Vg4PaxvuPvNxLzTOY2UhVaRWpD2nbH0WPNc0c/vrrsfAiCIAiCUFZk8SEIgiAIQlmRxYcgCIIgCGVl3Nl8FHnwGbhIdWFWiOpHvZR2rbKZEYGNdKfKpN1iMn227Wr9qGszPStqkMnsAlQJF1DPpQrJyiodKhpcqvPE9io8RC6v4xq3QeF2FJhAgApxWF6PudqayK7DKVB7GZ763RmmC6bN0tIbzIbAInUqw1f0HOaGy9zS8siepq+P6mSjKJ26ZTJ9qEHPW2Fq/alithp9Od0n3NU3WqGfLfP2AytMn3t9q3bj+7CbhtL2HH2NUqHxP6qqoSQ0pPLHELL9aOD31mKhqwsD+p0d3PUukW167r9I3T2i3ZbzC88nsrZmbS8TijCbDxZWH49nxVxSVVCPA9ukY9TE3cVzGZi07iIbA/7OBJANSp7p7HMFNseh5plsTBrYlqUoVD6d47AdUHWMhVBvmeEXD8WfJqL9XXQclsIM6H522HzHXUKxa7Cbp22NVuh5/UhvgsgKzKfYyet+joboc2+efLpffu/97UTmshQJk9Bc0FxXT8/TEPfLtrGTyOLd2nYjfZjeh8rR87gF/YysAHXX90zdHs/jvyvchViPg+pINZHV12q34L4kTfWQZ7kxPPQDEWYhAkYD2fkQBEEQBKGsyOJDEARBEISyMu7ULkfZKC5xJNuAxi5tLMOgjbIRWjbbagXq6opVG2GLbqOnlXbfKqEhOkqNqTLQlil3tTU9fQ2ujuBXLRWlkqgneJZJtpWHo8O6nsuO1eVIgEUUZccWpb0dgkCAuYFx1Qra8uaqHbwtWSiwSKDsrBmU1ba3h0bENZp0P9shFsqRuQx7rq6HmVdaBeqgVJa60BVQ9t7KCrotnGFtr4npLKWhmhoiy/bptgd5FuQi32hd5BGDS6ldhjuWjvbdUrjYDZR9z0H+kdWRGiILpuk788Izz/jld/+4lcg+dckn/fLsmXOIrHkSVTNMrtH97Cn6MD1PR76M2NQ9HgUmBYu7HgMDjdFcnkVcRSmmAyG63e3kqDogi+aGcJiFBEDvyLGeRsDS3/U82tqalrP9cuP084is98U3j3FmTSqt287nZj7/4Ban09SdFkeZtVjWX0/R+SaA1BVhFs6gs3evvoY3SGQRFSP1Cke/m1NjVNXTUKUjl4Yi/fQa+7SK6FCcqm4HgKpEcBRVMFiEVRQFl7spm6xeVa0jpU5tmU6vGdfnzWRYnwdpPYJCTDROpq7rTEt2XMjOhyAIgiAIZUUWH4IgCIIglBVZfAiCIAiCUFZOaZuPIlV3EIWLTR1mxyJbDZu53rEshibKemmxjJQhU5+HJVEkdhTFLo48M6sueyyEsIlcA02LuhMX69pRWGng4ND0Q33rT3X8barwU1jPyjwMbXZiZ5hht02Lh/NlYdqRXQOPAp5D9jxph2UMZa6LCeRuFh+gutxQDLnaAtXlxljo/gLSl3LdcgT1XZK52uayun1emD7LgQxte3WF7ufIZGqncKSvW7eVPTw+JLCY23EQb1Eemr6UHceJZMBFV2XRsSFSre+z+ey5RBZg9lbuId0Hf1x/iMh6euJ++Y2z9xLZWTNnkPqM6doFc0oDdYesr9RT5qQY1dnXIJsd3hsec23Ftlo2c/XNozFhsVQPIWZQhLMb57Ps2FAp27Chw+ErRdsaQOEE5i24kMgu3k7dn0vhoFDnyuO2NGxOQWOi4ND7Gkzp9zQYpPfBbcW8oL5OVRW1nznYrcdLIEzPU1GgfXBGm5afv2AfkfX1aHfw93fRfl63VT/LPSzjrhmlc0oINZ1nUnfRHKtYf9RHqC1JqELf58EumoYhj0xJXGbPxMz1oLZaj2ePjcPRQHY+BEEQBEEoK7L4EARBEAShrMjiQxAEQRCEsjLubD54+HBAdhRF+mvm822hOB9GkOr/PEfHX2AZnEExfa2F4iibwGNeoPbxqNZHCXyNGsuuodeFAZPKDJwO2+AX4Xp5nF+56KKoyMI228wPH4mtEqFFHFUU8YFWuUHCEFjc5kPxZ6vLLjP6SKFw1RmWStxNUv/5zoPaNiCfpjE4EnGty404LPYBC92fD+jr2Aazw0FxHLJZGjZZ4dThQRZGP0T7ri+l2x4IslDNKGR4IUvvkcdBwc/EYDFBSsXyGIkNyEjifOzf975frqik9jJzz9NxJZrnnE1kG197ntSTh1D/5amdVO9unfp9MHOEyDr7D5D6O/vf88tNLL7BJ2Itfrm99TQiO6NN10+bXEdkdpDOEwbS6VtsfgmhmOkuey9d1q+hiB5rmQwdW+mMHs8hFlrcZnFqXByym9tfZHUMjLkzadyIlXf+D1J/feM7MBT5nL4Xzy1tg+J6uu2eou+wQnNaiA4XcPLMAAzZhKSZ7VwQ28Sw4RpmNhezztbn2dsZJ7LNv9d9+fY7tO8GbfQMJlM7sQozxeq6EZUshkwBpYWwKuk8Uc3mov6EtmNLDtA5zTb1dy2L2sdMmVxD6iE0ZuMD3UQWqZwKJ4rsfAiCIAiCUFZGtPhYtWoVLFiwAKqrq6GhoQGuvfZa2LFjBzkmm83CsmXLoL6+HqqqqmDJkiXQ3d09xBkFQRAEQZhojEjt8sorr8CyZctgwYIF4DgOfOc734HPfOYzsH37dqis/Mjd54477oDf/OY38MQTT0AsFoPly5fDddddB6+//vqoNJjvCuNt4lJbxgB0+1kF6DakUdDbU4ZLt6qMID3WUlpumXTrKoS261yDbp1xd1raNopCagae4VWhjJjH3t5G7m1FLphDu/6WCrfu8dDRKJumYlHRcQZeAAADeNj0o8PVLtydVqFtSNdhGTLxwSyrbbKrl9Tr0DNqbGghskODeps2z8Idu2EajjmNtoINFvIZkEsbD/cO6D5wKHEAgAJTw/QN6jY0oxDKAAANbTrjbeL9XUTGXwsPjUODhbs/3vDqRSqZIY8s5ldPPu6X97x/LpGZSFUarKchry2XtqevXz8Ti6n3bEf3nXu4k8gGmY/hkU6thtkVptd8K6SzgrZMpWqXM6fq8fPJc2kY8tkzp5F6Fdo6NwymHg7gcPNEBDk21k1Dj60wm9PySF2RzVNXySCfcdDzC5q0Pclk71GPAwCoqaGuyKUYGEDZwNkr4nBVDwmhzlMroCzIPFsv+0XzkCrDcek1aqJI7ZJjKs9Bes19u7WfbOIwHRPJOEoBEKXzTXNDm18Ox6hLrGXROSSEVN3hID22fpJW42VytPO6emnYiMmTGv1yUz3LCFytzQ0Op+NEFqmg95xCYe25Gn40GNHi47nnniP1Rx55BBoaGmDLli1wySWXQCKRgIceegjWrFkDl112GQAAPPzww3DWWWfB+vXr4YILLhi9lguCIAiCMC45IZuPROKjYC91dR+tyrZs2QKFQgEWL17sHzNz5kxoa2uDdevWHfUcuVwOkskk+RMEQRAE4dTluBcfnufB7bffDhdddBHMnj0bAAC6urogGAxCDcu42djYCF1dXUc9z6pVqyAWi/l/ra2tRz1OEARBEIRTg+N2tV22bBls27YNXnvttRNqwMqVK2HFihV+PZlMllyAcFdbrPI7plYK6StN5mbkubrumSxlcVHacd1tJtMth5C+NMvsFDxyntKtxcfaQWY7AsiuhNt8cE9kY2hZqe9xQ4FSrUUZ48E06ZFeUerl4ekOi92mmcshsuvg+uIQSiWeHaDuh5UFet7mWq0fDdn0dchndT8fOhInsiRXmaOU5Dw1dTat22MZLKQxerYec3stMB31INLBRsO0rdPROzO4fz+ReRnq0qcUDqXtMdnQNh+lhg9/Xseyv8K89cZWv/z2m28TGQ67HYlSXfunPnslqduxGr+cOESN3C10mzazsQimaNzrRH+PX3ZcGp46W6ttHNID9B+qgx/oUOPb3niLyBacezGpX7BwgV9ubqYumNFq/WxjlbQfK5mLdT6l78Vw6LEWetdyitosOcz2KBzQ7pr799Hw4elBbfPR3ELnZpfnUygBfkVcnpOdGYFUhJFLKPuVCkfQ+81c1w327hmofRVsHg2ivkxnad81MLfYIyltM5PKUvuQujb92xFM0X62QfdrgEZ3gAyfG9F7WWXTg5MJPUZdZqfFMjZAT49+frEYtQ1zXN13kQrasUf6+0g9ldVy16TnARaO4ng4rsXH8uXL4dlnn4VXX30VWlq0kVVTUxPk83mIx+Nk96O7uxuampqOeq5QKASh0CjciSAIgiAI44IRqV2UUrB8+XJ46qmn4KWXXoL29nYinzdvHgQCAVi7dq3/2Y4dO2D//v3Q0dExOi0WBEEQBGFcM6Kdj2XLlsGaNWvgmWeegerqat+OIxaLQSQSgVgsBrfccgusWLEC6urqIBqNwje/+U3o6OgYNU+XIrULqprHcrVF4qJtfOSjZbKomHaAZZwNIpUN70EUXTPA3EqzyNe1KFJrkWoHRaHk+44FvbXHz6P4NmhRxFFyEXSe4jy2GOzt5vElK3IXtVlbDeYym84Pb73LvZLdAt2jTKNt44LNXPHSWmam6XZuJXObxi67BXbRxiq99ZpJ0+3Unl7qrnnE065xlQnqQgfIy9GtpNu5veiarkV3ACe5zD3S1FvMh4/QrK1nTtURB6OT6S5jvJeqIFRIPxPX4JFkkUs1V+l5Q6u+ity4R+Bsq5ArsirW0/lkEjTrcDZD+2fRFVf45Xd+v57I8n06gyjOigoAcLinh9QN9D+ZxeeJQa1qSeTitH0RrRZK9NIt7H2H9pD6K1te8cszzzifyE4/fbZfbm1sJLIpzN24aZLenq+JDZ0ZO1Sg37M9OkY/2LXRL//3878hsgo0RlrbaGTLtvZpMFxMNL4ViyBqWXS8eMhF3nXomHDRHMtVkx7PDo5U6AZTp2dR9uDsIFXJhJlq41CXHj8VlTX0PCl0LAuvEERhGYwkfdcKNv19MNBL1NdH3+90ZujoxlWVVCViIhf9HHudwugZGEx3Eo2xbO4o0282P/rB0Ed0xgcffBAAAC699FLy+cMPPww333wzAADcf//9YJomLFmyBHK5HFxxxRXwox/9aFQaKwiCIAjC+GdEi4/h5GsIh8OwevVqWL169XE3ShAEQRCEUxfJ7SIIgiAIQlkZd1ltPabjw6pmM1B6LYW9QF2mW3ZB6+McZjsSVFS37KFji8NT6++GTGo7klfIrZLHCy+yucBpZKmu0MzrtiuD6+G5DcjwfG257QgPaYzDq/PEtFjfb7K2usyFrsiOYAg8dh859tzxfbKkv5BLaNdSI89d+ujBBRTS3AjRew4jd+yWOpql1EnQkMZ9PVonnDLpeUIBrUv1PNp5OaR3DdZR97pKlrITuw2mc9Q9tC+rdcvVzc1E1ruNun0aDuo73nnkQObuzMQeccsd+jTHxETTEHvuOGs0E8EgswG59tqr/fKstjOIzBnQbspdfdR9du8HNBz9/t27/TJPrjylpcEv79q1l8iyaZ0t1/Po88mzMP+prHZf7Wcuu/vj2j7ktNNmEVltFU0BUBvT47KpgdoTtTdrt+AZzHW0++C7pP7MU4/45Xff205kIWQe8hbLhtv2CWoD0jz3szAUHx7U74gdYOHvWT/bln7u3ORNofFbNN8xGyacpiFQNDfpeT3Espzv30ff76paZCtWQe3G0mhuspjdlmXX6nKevs8B5lqfRfYhSYem+LAi+rt5oL8r/VmarRc87d5r5Gg/5wu6v/oNOkYra2n/5FF7wBj9fQrZ+RAEQRAEoazI4kMQBEEQhLIiiw9BEARBEMrK+Lf5wIE+WFyNolgaSIddnN4e6a9ZamrlUZ1aAIVnNnkIahSi22RhnHHrMsz2gEUlJ3E3eEtDOK6FQc9TMpp5UWgRFOfD5Fdhp0HfDXCjDzSMPGYn4Cm+vh1e2G2HPZ8Cu88g0t86R2jMAhVHqdWZrh1YXeFnybsA2eWEbaqvPaORplOPxbW+v3Own8hc1CdpFsrbC2i9c46lPc8E6Hi2DF1nwwf2I5uT05nfv8siCFvIzkW5rD9w7A4+tlkd69N5XI/heMb54PYVaHvI+85O2cfCyEfRoWd/8pNEls7qOAn5HA03P5CkMTn6e3WMhVQqTpuK7Mp++6sXiOz9D3R7rBDVn0drp5G6GdBxYXJs4HV16xgyg6yt9ZPpWK/NzPXLG3dRWWutbs/nzqM2DfE9NIz95j/oet8ROt9Vo3fYZPZM+zpp311fwuYDhw9XLFiQYu83nv+K5kbUX6rA53j6k6bI8GHxgJQed13dKSajx9Y0aNuNLJtlI6aeG0JBGk/FgRq/nDfCRFbJDF2qKlHKiACzT8nrfs4V6Bzisj4wTfy+09+ywTyaN3PUHu9Q/xFSr67RfVARYUsFOjUdF7LzIQiCIAhCWZHFhyAIgiAIZWXcqV14+GUDbwPyrd4iVYYaSkQy13J3La9InYMvycIEB/QWnMG2KG20pV2c9bMorSy6hjWUCAweTpi13SD3BSUovU2O3TwN4FvjqA+Yiy7fjh9urlOeqZa7RquUvma+m24XBnO6fR676QJ3zcPuokylh1USXIUXNeiYCEa1yyPfFv0wq7fDMyxzbgy50yrmfpitoNu0gE5rm/T6/SjUeFMVc5OO0K1XL6tdVJmnNumPYndwiuM4Ry1/9N3hq13MkL5Pz6OqJ7zDzV+ZQ/toyPI1P/l3v7z42i8Q2axzzvHLtVV0azw3ECf1LAqlbzEXw0hYq0s+e+ViIuvq1C6zb+2g7qp5j7pOBsL6mXT3076LRrQ7b2MDdZuuqZ1E6g2N1VpWw66RPeiXX32RhpsPZqmaIX4EuWD20zkkizSOoQibG0eQvdhGagbu4s1TY9goM7XBxhLJDM30jywSO1i2Pi/X0GSRCjY9SN/ZadPoGFEB3bdOjrUdubZGbPq9gbyWpWz6PvNwBjWV6L5y1NXXzOv7CDN3Z9ek9XwBqUMD9JrV1XqeKvC0uimmrnX0exAOjH7yV9n5EARBEAShrMjiQxAEQRCEsiKLD0EQBEEQyso4tPmg+lED6b6PZUdBatxWAkuZrYaraDfZ2M2TtcciSnSmL7a1jq3gUp193uFh45E7GXA7Cn19i9mcmMwWgPiaMbCbMrfN4C51FnJJVcxd1cP2GCaXMZuPYaqIPWZDoLLUFmCwN64vOUjT3YcsrefMc1dSfh30ictsHIg9CLcnYucNIduAKSjkNQCAg+yATIOHyte65oFskshOO+0sUo8j25Z8ih4bq9C2CJEIdavkN42rxc8DuXh73J6IHuwiGxme9nxE4O+y5449MIsSELCx/ebWN/zy3i6akvyTl17ml6/8zJVE1tJMQ4Qnj+h+Xv+H14nswF7tvhpm9g8NKLy5ycKrJ3qoDl+BvsbB/b1EFtyjw7snu08nsvpm6uIdzuhQ8YrZcezc+ge/HMl2E9lZ58wmdQebGzCbrnweheNnzycYGv5PiIXtONj8y0MWYGMk/s7aKIR5bTUNG9+FXM4BAKoq9LsQoN7yUBjUtlh1k6lNgxFifYDGuvLoHIunpnSO/z7h89A5LOvQ97SvX48ZhxmoKGxHxg21XO6Sr+2JHKA37aB0DpVRKnNNavORKWg7l85+On5r6DA8LmTnQxAEQRCEsiKLD0EQBEEQysq4U7vwPTjH0Rn+evZ+QGQVVXQbaXKjdmFTUGJLmW3n4iy2AAAmSrNo8AijSM1gsGy4AXQsz1hacFkGUdQGg7lkGWgrzWZRVB3mlesiV7RSK02vOPwpqVkozKDHQg4aWA3Fstg6Lu9ntvc5FAX2DBJUtVLo12oHK8fcckNIvVVg7eHbvehWWMJZ0gMej7ha5EKsv1zBoixOjensonVB6vq2+5BWD/Rm6T1GmqibZcTSW8wFphKJoO3wwcNxIoMczYKJ0wAXPfdSqiYGfmcMphTxiuLyDo2H1S6s73CVRyUuGoeoPb2dnUT29GOP++Wdb28jsr/40hJSv/jiDr88ayZVff3uxbV++de//jWRbdywWbeNqRsrKpnLY1TfWICpKrMDWkXSvZO6kR/ZSzMUT86e55f7u6n6ZtfbO/xyaz29/vkLqZqhpVm7iGZZpN0jSb3lzwMWx2qo2qMUZKwxt/YAi/apUFZbk100hzI6B6sqiSxgUxVEFYr2aweYKsXR56lvqieyIwkauTWbQVGB2UyaKuj3K5uL0+tX6fkuGGYusgX6XjrItdVzqQoN0DzG1Y2ZFD22slKrfe0QdbPP5pC7M/vNsQNUDaQ83V4+j44GsvMhCIIgCEJZkcWHIAiCIAhlRRYfgiAIgiCUlXFn82EwPW93pw4h/PsXnieyKa3UhW7Ouef75WhtNZGFcIhnpupmiTYhhdypKoI8i6v+ssN0lRaKFR2wuM6Tnge7t3G3YLC0XjNkU11cnukDXTW0ryIWFenTPW7XUeo8Q9sGcN13cTbho5NJUj1mvp+6lprIzoO7CbslrsFDzJM6z9aLutJl95hn7m64exSzV4kg9+e6qhp6iSrt7pdzaXjsbuTWCQDQ0nqmvl6Y6uwTR7QrXHea6uwreKh6Y2ibDwPdJ3e35s/ZRLZIBuu7omyjJTDC+t0LWtQ2IZfGmVqZKzSbCzxk/8VfWoXcHN96YzOR7d2zi9TffffP/fINN3yZyJYuvckvL1q0kMh++cun/PIL//0ikcUT1AW0qWmKX55xUTuRdfX16O91URfHaJDq5Y906WP7e+k1cA8kMuz9Zi/xebO1S+/cOdQN9/FfaTuXfJ65wA/zfQYAYmtkBOiYtEJ8HtVybNcHAGCjdBeZPJ0nItX0vUihrMCxMLUPcbFdHRtbkQraz/2JuF9WQNujkFu1wVysse1GJQvZoBS18VKmvkahQG19DGQfwu0VbWbn4rn6WJOZahgoA6/rsnDvPEqDo98ZE+jv5WggOx+CIAiCIJQVWXwIgiAIglBWZPEhCIIgCEJZGXc2H4rp2jsP6FDEu7a/TWQDcaoDTfRpP/jLr7qKyMLIH1oVheSm+snBHFqzuVT/FwmjUNpMF+cgHWOI3UeI6cxzSJfKI6S7NtK1Az8P82VHuu9Sac49xe0mmE86iT5PffJt1B7FbEUsFndEcQOaIcDh0wEAgNkxWCVMCnCMCZYRHSzWByaKl2Eobp+CbSPYNdhDCbv6Qhm2pveQ/pjbRlRV6nE3RdEYKF15pj9GdgsxFtY61aVtnzwW6rzAxi822jH5mKCGQFTEzoLrJu/o4Zt8QLBWx5iwWUwbFUF6aaNEWwHAKOj+cVKDROahUNEWa1wikSD1XzyuY4Ls2rmTyK7/iy/65Y4LLiSy5d/4ll+ef/4iIvvNs0+T+rZ39Vw1rb2FyK76zGK//MyTzxHZ7g8OkrrydEwQbqdlo+k9z2IV5T3azw6qT5veRmSntUz2yzt30LD1hw/TviuFQvFMWFZ4MJjtGn5EeZZaIRDS86rDYinZLI5ONqufe7ZA5+Pa+lq/nByg92GwudtA/VdgIeZdJ+OXLTY3mq6+0WyKfs9mdi8uxPU1CjzOx9A2bpEIm29cbUtSyFBbOUNp2w3XpPYxIfa7Z6EgOzyu0GggOx+CIAiCIJSVES0+HnzwQZg7dy5Eo1GIRqPQ0dEBv/3tb315NpuFZcuWQX19PVRVVcGSJUugu7u7xBkFQRAEQZhojEjt0tLSAvfddx/MmDEDlFLw05/+FK655hp488034eyzz4Y77rgDfvOb38ATTzwBsVgMli9fDtdddx28/vrrxz75MOEqEQNt9wbYXl4Py2yZy+ktOI+5H2IXXsW21It2e9HWeYbtFobQlhwPp5tx9Pdch20XWnQLDF9Usa3xPNrid1hY4gDLnAhou7BkuOyirK20DxRSrbjMXxVngzXYfRhFW/7D274rsHDqYbZtjFVI/L4UkTEVGrsvr4DCxrNtUNInPDtvkQ4CXZONw3C1DvFcyGSILIDaM9lmocXZJQzkThscpP832AN6ezXAQtxz1Qoe3gYMrVrhqrjiWx46K3LReCpBeLIObc0z5wZRnWfrjbB+9pBbZX6QbqMPJrRLanaAujG6A9TF2UGu9JvWbyKyA3u0+/OVV24nsmuuudYvL/6zy4ls7pyZpP7i2hf88uvrX2Nt1e25/sYbiezxnz9J6h+8uwPV6JjIo3cvx9Qsnk1VfD0JPW+0GNQltRKHMOfvvsv0qiUwA/q7VoCND5tnudXtjVTTEOE2Ugc4zK1dMd9SK6jPk8rQOQWnQcjmqMxkbqjBQBgdO0CviVQ/Cqi6D6uagKmIisKre7oNPEs0/t3j7wh/hw1TnzfH2uoZ+h1xmFs7y9QBQWRCkOLh3keBES0+rr76alK/99574cEHH4T169dDS0sLPPTQQ7BmzRq47LKP0lc//PDDcNZZZ8H69evhggsuGL1WC4IgCIIwbjlumw/XdeGxxx6DVCoFHR0dsGXLFigUCrB4sTaWmjlzJrS1tcG6deuGPE8ul4NkMkn+BEEQBEE4dRnx4uOdd96BqqoqCIVCcOutt8JTTz0Fs2bNgq6uLggGg1BTU0OOb2xshK6uriHPt2rVKojFYv5fa2vriG9CEARBEITxw4hdbc8880zYunUrJBIJ+OUvfwlLly6FV1555bgbsHLlSlixYoVfTyaTJRcgR/ppuOHKau2mVxmNEdm+PXtJff7Fn/LLOZY2OjWodXXVMXoexdyMTKRjc5m+DaejDrGQ6QEU2tbj+jXmYhgJalevNHM1w2nrC+wRVgLVXwdRiOy0y+8DwV1tnaHDiXObGBPpIG1mtwAm0116w0vNbGZZ+OUA1S3jW+HPAIdC58/OZTYfOIR4kU0M8i9m0fDBYG6NeXSeQCXVmYfQs8z1U3uDCLpGmP0rUMPcBjNppK9N0x1CC4UhDzF3P4PdM/F25q7QI/CRJTYfI7Dx4ARi2v2Pe/5alh5PVdU0ffsk9D0AAAeF2k4PVhFZRVbbi2ST9Dw9zH0116315NyFuKtLG9A/8sgjRLb1rTf98ldvXkpkn77kElL/6s03++X5CxcQ2X+/oO1BTptC3XC/ddsyUl99/w/98p5du4msukr3z7zzacj09hlnkfreLjSegjVElk4jN2WTDxhqO1KKQEj3JXfBN3mofDQQLIMdjN49i9m8uczeCU9yaWZvlcnj8OqUzCA91g5ou5OAxWyNDG1jYbGJwjL0eRSz+fDcEnNTkWkctt2jklSKuuRblu6DPLOJcR1tk4dDJAAAFIC6F1sopES+QG1ZRoMRLz6CwSBMnz4dAADmzZsHmzZtgh/84Adw/fXXQz6fh3g8TnY/uru7oampacjzhUIhCIVCQ8oFQRAEQTi1OOE4H57nQS6Xg3nz5kEgEIC1a3USoh07dsD+/fuho6PjRC8jCIIgCMIpwoh2PlauXAlXXXUVtLW1wcDAAKxZswZ+97vfwfPPPw+xWAxuueUWWLFiBdTV1UE0GoVvfvOb0NHRIZ4ugiAIgiD4jGjx0dPTAzfddBN0dnZCLBaDuXPnwvPPPw9/9md/BgAA999/P5imCUuWLIFcLgdXXHEF/OhHPxrVBm987VVS37LhD355326aGrsiQvW+iX4dbn3rpo1ENu/Ci/1yrKaWyLwSsSlctnmUxjEUBlmsirAWWszGw/KorUY1UkVxX/ocSlfuulQByHWgoYC+TpbZcZgkfDhLic7SqVso3DC3C3AcZO/A4mpYFo9dweN+HB2Th9ww6TWx3pOf8XjtD4rtHVB6eTYElEv7q4DCnUej1BYhPahtCJRL9b4mDqOfpXrmoMn9+fWx+TTVweLQ4maY6uEdptAuUGMfKiwR24T3j1JDb5yO5BkEw0jXzIwBKlA8imgVVc9GIiwHeETbcgTCLDy20eiXTUXtAsIGjSPxfnyrX1ZZFlcI3TILtwBb33zLL//zof9FZNvepjFBrv78n/vl2WfPIbKWFm3nkclSfX5VJR1b0btW+uW9zMatYZKOn3LGmdOJLMvG4TvvH/DLDgtik8noN4yZCYBToO0rBZ4KeN8pbnWBqkpxexAkY/FLeLp5bA9msLGFh6jJGhRmMZpyWWzXQZsaDKKUGnzeJO8wHXfcvonE62C2Rgaye/GK7Nj4fKzb6jLjwpyjbZYqKpidIZs3FIoZZXg8ftSJM6LFx0MPPVRSHg6HYfXq1bB69eoTapQgCIIgCKcukttFEARBEISyMu6y2h7cS93J9n2gs07Gj9AstoMWdUd8+hc/98sdF32KyBZ98tN+ucDC3hZl90T7ZQbbLiyg7busycJso3C+RSHKc3TL3UVuWaaiW8hYXVJgrlQFltUWu7Z6Lt82R2oXdo8FlnXXQee1TNYe1AdOlobhdVx6rDvM7IjFyVaZWghdk90WuOjL5khChLOLeujoYlUTrYfq9Xa4YqqmAaTuq7DZNdAWdyHPxh3718BAqifL41u4ehzwZ1dgKqsC6kuT9SvO8svHBK+T3WeulhqB2iUUQi7FTNUUsPT4CQWo2sU0aD9jr7lImKonPHSfPKttsJ1uKfci19tEVy+R2ch9nmeJNtCz5Dmt/vNnj5L6tu1/9Mt/8cUvENnFF+tsuZPrJxEZH3ezZ8/1y2cz9Q0glR7TBkDPERqyoH6yvk4/ywaeSmmVsGLb+CN5zti102AqO8XHD1I7mGy+ATQmgIdBYG65WGVe5M6L3XLZbURZ2IYk6N8ST1EVOR4TRpGqFKG4Qy/LAI41nkXqJNwfPMT90M+Eh0XwPH0f+Rydb2ojVO0SQuqkrszw1OUjQXY+BEEQBEEoK7L4EARBEAShrMjiQxAEQRCEsmKoE4mL/DGQTCYhFovBXXfdJZFPBUEQBGGckMvl4L777oNEIgHRaLTksbLzIQiCIAhCWZHFhyAIgiAIZUUWH4IgCIIglBVZfAiCIAiCUFZk8SEIgiAIQlk56SKc/sn5JpcbfsIiQRAEQRDGlj/9bg/Hifakc7U9ePAgtLa2jnUzBEEQBEE4Dg4cOECyMx+Nk27x4XkeHDp0CJRS0NbWBgcOHDimv/BEJJlMQmtrq/TPEEj/lEb6pzTSP6WR/hmaidw3SikYGBiA5uZmMHlCIcZJp3YxTRNaWlogmfwoAU40Gp1wD3AkSP+URvqnNNI/pZH+KY30z9BM1L6JxWLHPgjE4FQQBEEQhDIjiw9BEARBEMrKSbv4CIVC8Pd///eS32UIpH9KI/1TGumf0kj/lEb6Z2ikb4bHSWdwKgiCIAjCqc1Ju/MhCIIgCMKpiSw+BEEQBEEoK7L4EARBEAShrMjiQxAEQRCEsiKLD0EQBEEQyspJu/hYvXo1TJs2DcLhMCxatAg2btw41k0qO6tWrYIFCxZAdXU1NDQ0wLXXXgs7duwgx2SzWVi2bBnU19dDVVUVLFmyBLq7u8eoxWPLfffdB4ZhwO233+5/NtH758MPP4S//Mu/hPr6eohEIjBnzhzYvHmzL1dKwT333ANTpkyBSCQCixcvhl27do1hi8uH67pw9913Q3t7O0QiEfjEJz4B//iP/0iSYk2k/nn11Vfh6quvhubmZjAMA55++mkiH05f9Pf3w4033gjRaBRqamrglltugcHBwTLexcdHqf4pFApw5513wpw5c6CyshKam5vhpptugkOHDpFznMr9M2LUSchjjz2mgsGg+o//+A/1xz/+Uf3VX/2VqqmpUd3d3WPdtLJyxRVXqIcfflht27ZNbd26VX32s59VbW1tanBw0D/m1ltvVa2trWrt2rVq8+bN6oILLlAXXnjhGLZ6bNi4caOaNm2amjt3rrrtttv8zydy//T396upU6eqm2++WW3YsEHt3r1bPf/88+r999/3j7nvvvtULBZTTz/9tHrrrbfU5z//edXe3q4ymcwYtrw83Hvvvaq+vl49++yzas+ePeqJJ55QVVVV6gc/+IF/zETqn//6r/9S3/3ud9WTTz6pAEA99dRTRD6cvrjyyivVOeeco9avX69+//vfq+nTp6sbbrihzHfy8VCqf+LxuFq8eLF6/PHH1XvvvafWrVunFi5cqObNm0fOcSr3z0g5KRcfCxcuVMuWLfPrruuq5uZmtWrVqjFs1djT09OjAEC98sorSqmPBnwgEFBPPPGEf8y7776rAECtW7durJpZdgYGBtSMGTPUCy+8oD71qU/5i4+J3j933nmnuvjii4eUe56nmpqa1Pe//33/s3g8rkKhkPr5z39ejiaOKZ/73OfU1772NfLZddddp2688Ual1MTuH/7jOpy+2L59uwIAtWnTJv+Y3/72t8owDPXhhx+Wre3l4GiLM87GjRsVAKh9+/YppSZW/wyHk07tks/nYcuWLbB48WL/M9M0YfHixbBu3boxbNnYk0gkAACgrq4OAAC2bNkChUKB9NXMmTOhra1tQvXVsmXL4HOf+xzpBwDpn1/96lcwf/58+OIXvwgNDQ1w3nnnwU9+8hNfvmfPHujq6iL9E4vFYNGiRROify688EJYu3Yt7Ny5EwAA3nrrLXjttdfgqquuAgDpH8xw+mLdunVQU1MD8+fP949ZvHgxmKYJGzZsKHubx5pEIgGGYUBNTQ0ASP9wTrqstn19feC6LjQ2NpLPGxsb4b333hujVo09nufB7bffDhdddBHMnj0bAAC6urogGAz6g/tPNDY2QldX1xi0svw89thj8MYbb8CmTZuKZBO9f3bv3g0PPvggrFixAr7zne/Apk2b4Fvf+hYEg0FYunSp3wdHe9cmQv/cddddkEwmYebMmWBZFriuC/feey/ceOONAAATvn8ww+mLrq4uaGhoIHLbtqGurm7C9Vc2m4U777wTbrjhBj+zrfQP5aRbfAhHZ9myZbBt2zZ47bXXxropJw0HDhyA2267DV544QUIh8Nj3ZyTDs/zYP78+fDP//zPAABw3nnnwbZt2+DHP/4xLF26dIxbN/b84he/gEcffRTWrFkDZ599NmzduhVuv/12aG5ulv4RjptCoQBf+tKXQCkFDz744Fg356TlpFO7TJo0CSzLKvJI6O7uhqampjFq1diyfPlyePbZZ+Hll1+GlpYW//OmpibI5/MQj8fJ8ROlr7Zs2QI9PT1w/vnng23bYNs2vPLKK/DDH/4QbNuGxsbGCd0/U6ZMgVmzZpHPzjrrLNi/fz8AgN8HE/Vd+9u//Vu466674Mtf/jLMmTMHvvKVr8Add9wBq1atAgDpH8xw+qKpqQl6enqI3HEc6O/vnzD99aeFx759++CFF17wdz0ApH84J93iIxgMwrx582Dt2rX+Z57nwdq1a6Gjo2MMW1Z+lFKwfPlyeOqpp+Cll16C9vZ2Ip83bx4EAgHSVzt27ID9+/dPiL66/PLL4Z133oGtW7f6f/Pnz4cbb7zRL0/k/rnooouKXLN37twJU6dOBQCA9vZ2aGpqIv2TTCZhw4YNE6J/0uk0mCadAi3LAs/zAED6BzOcvujo6IB4PA5btmzxj3nppZfA8zxYtGhR2dtcbv608Ni1axe8+OKLUF9fT+QTvX+KGGuL16Px2GOPqVAopB555BG1fft29fWvf13V1NSorq6usW5aWfmbv/kbFYvF1O9+9zvV2dnp/6XTaf+YW2+9VbW1tamXXnpJbd68WXV0dKiOjo4xbPXYgr1dlJrY/bNx40Zl27a699571a5du9Sjjz6qKioq1M9+9jP/mPvuu0/V1NSoZ555Rr399tvqmmuuOWVdSTlLly5Vp512mu9q++STT6pJkyapb3/72/4xE6l/BgYG1JtvvqnefPNNBQDqX//1X9Wbb77pe2sMpy+uvPJKdd5556kNGzao1157Tc2YMeOUcSUt1T/5fF59/vOfVy0tLWrr1q1kvs7lcv45TuX+GSkn5eJDKaX+7d/+TbW1talgMKgWLlyo1q9fP9ZNKjsAcNS/hx9+2D8mk8mob3zjG6q2tlZVVFSoL3zhC6qzs3PsGj3G8MXHRO+fX//612r27NkqFAqpmTNnqn//938ncs/z1N13360aGxtVKBRSl19+udqxY8cYtba8JJNJddttt6m2tjYVDofV6aefrr773e+SH4uJ1D8vv/zyUeebpUuXKqWG1xeHDx9WN9xwg6qqqlLRaFR99atfVQMDA2NwN6NPqf7Zs2fPkPP1yy+/7J/jVO6fkWIohcL5CYIgCIIgfMycdDYfgiAIgiCc2sjiQxAEQRCEsiKLD0EQBEEQyoosPgRBEARBKCuy+BAEQRAEoazI4kMQBEEQhLIiiw9BEARBEMqKLD4EQRAEQSgrsvgQBEEQBKGsyOJDEARBEISyIosPQRAEQRDKyv8Di42wE6CKjF8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane horse ship  bird \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "298284b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "#         x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# net = model\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730d1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a037c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.234\n",
      "[1,  4000] loss: 1.865\n",
      "[1,  6000] loss: 1.671\n",
      "[1,  8000] loss: 1.575\n",
      "[1, 10000] loss: 1.499\n",
      "[1, 12000] loss: 1.455\n",
      "[2,  2000] loss: 1.386\n",
      "[2,  4000] loss: 1.347\n",
      "[2,  6000] loss: 1.347\n",
      "[2,  8000] loss: 1.348\n",
      "[2, 10000] loss: 1.317\n",
      "[2, 12000] loss: 1.283\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab6dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f751d60f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  cat   ship  ship  plane\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac581da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf564aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1309a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a75ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2664d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  cat   ship  plane plane\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f659620",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bad37307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: plane is 62.7 %\n",
      "Accuracy for class: car   is 76.6 %\n",
      "Accuracy for class: bird  is 47.3 %\n",
      "Accuracy for class: cat   is 28.6 %\n",
      "Accuracy for class: deer  is 33.9 %\n",
      "Accuracy for class: dog   is 42.0 %\n",
      "Accuracy for class: frog  is 81.5 %\n",
      "Accuracy for class: horse is 64.8 %\n",
      "Accuracy for class: ship  is 64.6 %\n",
      "Accuracy for class: truck is 53.3 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images.to(device))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be1e0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy: 55.5 %\n"
     ]
    }
   ],
   "source": [
    "#print accuracy of whole\n",
    "print(f'Total Accuracy: {100*sum(correct_pred.values())/sum(total_pred.values()):.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56685351",
   "metadata": {},
   "source": [
    "# Eigen Arithematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fddf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_output = []\n",
    "def hook_fn(module, input, output):\n",
    "    view_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4d59716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 1, 7])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfe2a4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=10, bias=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.classifier[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c2de703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ = 3\n",
    "images_class = []\n",
    "labels_class = []\n",
    "images_nonclass = []\n",
    "labels_nonclass = []\n",
    "\n",
    "for data in trainloader:\n",
    "    if len(images_class) == 250 and len(images_nonclass)==250:\n",
    "        break\n",
    "    images, labels = data\n",
    "    for label, image in zip(labels,images):\n",
    "        if label == class_:\n",
    "            images_class.append(image)\n",
    "            labels_class.append(label)\n",
    "        else:\n",
    "            if len(images_nonclass)<250:\n",
    "                images_nonclass.append(image)\n",
    "                labels_nonclass.append(label)\n",
    "images_class = torch.stack(images_class)\n",
    "images_nonclass = torch.stack(images_nonclass)\n",
    "\n",
    "labels_class = torch.stack(labels_class)\n",
    "labels_nonclass = torch.stack(labels_nonclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2091abe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 3, 32, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a99fc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 3, 32, 32])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_nonclass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9bfede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again no gradients needed\n",
    "view_output\n",
    "hook = net.classifier[6].register_forward_hook(hook_fn)\n",
    "with torch.no_grad():\n",
    "    outputs = net(images_class.to(device))\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffbf4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(images_class, idx):\n",
    "    indices = view_output[0][idx].max(1)[0].max(1)[0].topk(10)[1].numpy()\n",
    "    imshow(images_class[idx])\n",
    "    \n",
    "    channels = view_output[0][idx].unsqueeze(1)\n",
    "    \n",
    "    for i in range(channels.shape[0]):\n",
    "        channels[i] = (channels[i] - channels[i].min())/(channels[i].max() - channels[i].min())\n",
    "    imshow(torchvision.utils.make_grid(channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf227189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 3, 32, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa03100c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mvisualise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [35], line 2\u001b[0m, in \u001b[0;36mvisualise\u001b[0;34m(images_class, idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualise\u001b[39m(images_class, idx):\n\u001b[0;32m----> 2\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mview_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m10\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m     imshow(images_class[idx])\n\u001b[1;32m      5\u001b[0m     channels \u001b[38;5;241m=\u001b[39m view_output[\u001b[38;5;241m0\u001b[39m][idx]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    idx = np.random.randint(200)\n",
    "    visualise(images_class, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6025e1",
   "metadata": {},
   "source": [
    "# Remove the direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aabbbd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(view_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddf51bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bf6ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_direction = view_output[0].mean(0)\n",
    "view_output[0] = view_output[0] - view_output[0].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39850c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchtensor = view_output[0].flatten(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f344fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d05fe478",
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,v = torch.svd(torchtensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44007110",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_eigens = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7c1b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_direction = u.T[:top_eigens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62593dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_direction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8041abc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "unflatten: Provided sizes [2, 10] don't multiply up to the size of dim 1 (10) in the input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m unflatten \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mUnflatten(\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m important_direction \u001b[38;5;241m=\u001b[39m \u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimportant_direction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/flatten.py:138\u001b[0m, in \u001b[0;36mUnflatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflattened_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:992\u001b[0m, in \u001b[0;36mTensor.unflatten\u001b[0;34m(self, dim, sizes)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sizes, OrderedDict) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(sizes, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sizes[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))):\n\u001b[1;32m    991\u001b[0m     names, sizes \u001b[38;5;241m=\u001b[39m unzip_namedshape(sizes)\n\u001b[0;32m--> 992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unflatten: Provided sizes [2, 10] don't multiply up to the size of dim 1 (10) in the input tensor"
     ]
    }
   ],
   "source": [
    "unflatten = torch.nn.Unflatten(1, (2,10))\n",
    "important_direction = unflatten(important_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14268a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_direction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90ec7bf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "conv2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_dup\n\u001b[1;32m     25\u001b[0m views \u001b[38;5;241m=\u001b[39m []   \n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mTrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medit_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_directions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tr:\n\u001b[1;32m     27\u001b[0m     out \u001b[38;5;241m=\u001b[39m net(images_class)\n\u001b[1;32m     28\u001b[0m _, predict_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(out, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/baukit/nethook.py:69\u001b[0m, in \u001b[0;36mTrace.__init__\u001b[0;34m(self, module, layer, retain_output, retain_input, clone, detach, retain_grad, edit_output, stop)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m=\u001b[39m layer\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretain_hook\u001b[39m(m, inputs, output):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m edit_output:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/baukit/nethook.py:368\u001b[0m, in \u001b[0;36mget_module\u001b[0;34m(model, name)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m name:\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m m\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(name)\n",
      "\u001b[0;31mLookupError\u001b[0m: conv2"
     ]
    }
   ],
   "source": [
    "from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar, TraceDict\n",
    "views = []\n",
    "def remove_directions(output,layer):\n",
    "#     print(output)\n",
    "    output_dup = output.detach().clone()\n",
    "    for i in range(important_direction.shape[0]):\n",
    "        for j in range(output.shape[0]):\n",
    "            magnitude = torch.norm(important_direction[i])\n",
    "            dot = torch.dot(output_dup[j], important_direction[i])\n",
    "            dot = dot/magnitude\n",
    "            mean_dot = torch.dot(mean_vector, important_direction[i])\n",
    "            mean_dot = mean_dot/magnitude\n",
    "            \n",
    "            magnitude = torch.norm(important_direction[i])\n",
    "            dot = torch.dot(output_dup[j].flatten(), important_direction[i].flatten())\n",
    "            dot = dot/magnitude\n",
    "            \n",
    "            mean_dot = torch.dot(mean_direction.flatten(), important_direction[i].flatten())\n",
    "            mean_dot = dot/magnitude\n",
    "            \n",
    "            output_dup[j] = output_dup[j] - important_direction[i]*dot + important_direction[i]*mean_dot\n",
    "#     print(output_dup)\n",
    "    views.append(output_dup)\n",
    "    return output_dup\n",
    "views = []   \n",
    "with Trace(net, 'conv2', edit_output=remove_directions) as tr:\n",
    "    out = net(images_class)\n",
    "_, predict_y = torch.max(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d062df",
   "metadata": {},
   "outputs": [],
   "source": [
    "(predict_y == labels_class).sum()/len(labels_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec69173",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(images_class)\n",
    "_, predict_y = torch.max(out, 1)\n",
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(predict_y == labels_class).sum()/len(labels_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbdbe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7852cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar, TraceDict\n",
    "views = []\n",
    "def remove_directions(output,layer):\n",
    "#     print(output)\n",
    "    output_dup = output.detach().clone()\n",
    "    for i in range(important_direction.shape[0]):\n",
    "        for j in range(output.shape[0]):\n",
    "            dot = torch.dot(output_dup[j].flatten(), important_direction[i].flatten())\n",
    "            output_dup[j] = output_dup[j] - important_direction[i]*dot\n",
    "#     print(output_dup)\n",
    "    views.append(output_dup)\n",
    "    return output_dup\n",
    "views = []   \n",
    "with Trace(net, 'conv2', edit_output=remove_directions) as tr:\n",
    "    out = net(images_nonclass)\n",
    "_, predict_y = torch.max(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf20aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ca2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(predict_y == labels_nonclass).sum()/len(labels_nonclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(images_nonclass)\n",
    "_, predict_y = torch.max(out, 1)\n",
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db694d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "(predict_y == labels_nonclass).sum()/len(labels_nonclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        views = []\n",
    "        with Trace(net, 'conv2', edit_output=remove_directions) as tr:\n",
    "            outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a544956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        views = []\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_big = torch.randn(7, 16)\n",
    "u, s, v = torch.svd(a_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_big.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60243420",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd9970",
   "metadata": {},
   "source": [
    "# Consolidated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53ff5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f12d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar, TraceDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "#         x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "PATH = './cifar_net.pth'\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04a8e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_class(class_,trainloader):\n",
    "    images_class = []\n",
    "    labels_class = []\n",
    "    images_nonclass = []\n",
    "    labels_nonclass = []\n",
    "\n",
    "    for data in trainloader:\n",
    "        if len(images_class) == 250 and len(images_nonclass)==250:\n",
    "            break\n",
    "        images, labels = data\n",
    "        for label, image in zip(labels,images):\n",
    "            if label == class_:\n",
    "                images_class.append(image)\n",
    "                labels_class.append(label)\n",
    "            else:\n",
    "                if len(images_nonclass)<250:\n",
    "                    images_nonclass.append(image)\n",
    "                    labels_nonclass.append(label)\n",
    "    try:\n",
    "        images_class = torch.stack(images_class)\n",
    "        labels_class = torch.stack(labels_class)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    images_nonclass = torch.stack(images_nonclass)\n",
    "    labels_nonclass = torch.stack(labels_nonclass)\n",
    "    del(data)\n",
    "    return images_class, images_nonclass, labels_class, labels_nonclass\n",
    "\n",
    "def eval_top_eigens(class_idx, top_eigens, trainloader, testloader):\n",
    "#     print('Reading')\n",
    "    images_class, images_nonclass, labels_class, labels_nonclass = get_data_class(class_idx, trainloader)\n",
    "#     print('Done')\n",
    "    view_output = []\n",
    "    def hook_fn(module, input, output):\n",
    "        view_output.append(output)\n",
    "\n",
    "    # again no gradients needed\n",
    "    view_output = []\n",
    "    hook = net.conv2.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        outputs = net(images_class)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "    hook.remove()\n",
    "    torchtensor = view_output[0]#.max(2)[0].max(2)[0]\n",
    "    view_output[0] = view_output[0] - view_output[0].mean(0)\n",
    "    \n",
    "    torchtensor = view_output[0].flatten(1,-1)\n",
    "    \n",
    "    u,s,v = torch.svd(torchtensor.T)\n",
    "    \n",
    "    important_direction = u.T[:top_eigens]\n",
    "    unflatten = torch.nn.Unflatten(1, (16,10,10))\n",
    "    important_direction = unflatten(important_direction)\n",
    "    \n",
    "#     print(f'Removing Eigen Directions of class: {classes[class_idx]}')\n",
    "    removing_object = classes[class_idx]\n",
    "    def remove_directions(output,layer):\n",
    "        output_dup = output.detach().clone()\n",
    "        for i in range(important_direction.shape[0]):\n",
    "            for j in range(output.shape[0]):\n",
    "                dot = torch.dot(output[j].flatten(), important_direction[i].flatten())\n",
    "                output_dup[j] = output_dup[j] - important_direction[i]*dot\n",
    "        return output_dup\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred_edited = {classname: 0 for classname in classes}\n",
    "    total_pred_edited = {classname: 0 for classname in classes}\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "            # get edited scores\n",
    "            with Trace(net, 'conv2', edit_output=remove_directions) as tr:\n",
    "                outputs_ = net(images)\n",
    "            _, predictions_ = torch.max(outputs_, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions_):\n",
    "                if label == prediction:\n",
    "                    correct_pred_edited[classes[label]] += 1\n",
    "                total_pred_edited[classes[label]] += 1\n",
    "\n",
    "\n",
    "#     # print accuracy for each class\n",
    "#     for original, edited in zip(correct_pred.items(),correct_pred_edited.items()):\n",
    "#         accuracy_original = 100 * float(original[1]) / total_pred[original[0]]\n",
    "#         accuracy_edited = 100 * float(edited[1]) / total_pred[edited[0]]\n",
    "#         perc_changed = (accuracy_original - accuracy_edited)\n",
    "#         print(f'Accuracy for class: {original[0]:5s} \\t Original {accuracy_original:.1f} % \\t Edited {accuracy_edited:.1f} % \\t Change {perc_changed:.1f} %')\n",
    "    accs = []\n",
    "    cls = []\n",
    "    for original, edited in zip(correct_pred.items(),correct_pred_edited.items()):\n",
    "        accuracy_original = 100 * float(original[1]) / total_pred[original[0]]\n",
    "        accuracy_edited = 100 * float(edited[1]) / total_pred[edited[0]]\n",
    "        perc_changed = (accuracy_original - accuracy_edited)\n",
    "        accs.append(f'{perc_changed:.1f}')\n",
    "        cls.append(original[0])\n",
    "#         print(f'Accuracy for class: {original[0]:5s} \\t Original {accuracy_original:.1f} % \\t Edited {accuracy_edited:.1f} % \\t Change {perc_changed:.1f} %')\n",
    "    text = f'{removing_object} '\n",
    "    for a in accs:\n",
    "        text+=f'& {a} '\n",
    "    print(text + ' \\\\\\\\')\n",
    "    del images_class, images_nonclass, labels_class, labels_nonclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695371f",
   "metadata": {},
   "source": [
    "# Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d70636ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_class(class_,trainloader):\n",
    "    images_class = []\n",
    "    labels_class = []\n",
    "    images_nonclass = []\n",
    "    labels_nonclass = []\n",
    "\n",
    "    for data in trainloader:\n",
    "        if len(images_class) == 250 and len(images_nonclass)==250:\n",
    "            break\n",
    "        images, labels = data\n",
    "        for label, image in zip(labels,images):\n",
    "            if label == class_:\n",
    "                images_class.append(image)\n",
    "                labels_class.append(label)\n",
    "            else:\n",
    "                if len(images_nonclass)<250:\n",
    "                    images_nonclass.append(image)\n",
    "                    labels_nonclass.append(label)\n",
    "    try:\n",
    "        images_class = torch.stack(images_class)\n",
    "        labels_class = torch.stack(labels_class)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    images_nonclass = torch.stack(images_nonclass)\n",
    "    labels_nonclass = torch.stack(labels_nonclass)\n",
    "    del(data)\n",
    "    return images_class, images_nonclass, labels_class, labels_nonclass\n",
    "\n",
    "def eval_top_eigens(class_idx, top_eigens, trainloader, testloader):\n",
    "#     print('Reading')\n",
    "    images_class, images_nonclass, labels_class, labels_nonclass = get_data_class(class_idx, trainloader)\n",
    "#     print('Done')\n",
    "    view_output = []\n",
    "    def hook_fn(module, input, output):\n",
    "        view_output.append(output)\n",
    "    \n",
    "    # again no gradients needed\n",
    "    view_output = []\n",
    "    hook = net.conv2.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        outputs = net(images_nonclass)\n",
    "    hook.remove()\n",
    "    torchtensor_nonclass = view_output[0]\n",
    "    \n",
    "    # again no gradients needed\n",
    "    view_output = []\n",
    "    hook = net.conv2.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        outputs = net(images_class)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "    hook.remove()\n",
    "    torchtensor = view_output[0]#.max(2)[0].max(2)[0]\n",
    "    mean_direction = torch.cat([torchtensor_nonclass, torchtensor]).mean(0)\n",
    "    \n",
    "    view_output[0] = view_output[0] - view_output[0].mean(0)\n",
    "    \n",
    "    torchtensor = view_output[0].flatten(1,-1)\n",
    "    \n",
    "    u,s,v = torch.svd(torchtensor.T)\n",
    "    \n",
    "    important_direction = u.T[:top_eigens]\n",
    "    unflatten = torch.nn.Unflatten(1, (16,10,10))\n",
    "    important_direction = unflatten(important_direction)\n",
    "    removing_object = classes[class_idx]\n",
    "#     print(f'Removing Eigen Directions of class: {classes[class_idx]}')\n",
    "    def remove_directions(output,layer):\n",
    "        output_dup = output.detach().clone()\n",
    "        for i in range(important_direction.shape[0]):\n",
    "            for j in range(output.shape[0]):\n",
    "                magnitude = torch.norm(important_direction[i])\n",
    "                mean_dot = torch.dot(mean_direction.flatten(), important_direction[i].flatten())\n",
    "                mean_dot = mean_dot/magnitude\n",
    "                dot = torch.dot(output[j].flatten(), important_direction[i].flatten())\n",
    "                dot = dot/magnitude\n",
    "                output_dup[j] = output_dup[j] - important_direction[i]*dot + important_direction[i]*mean_dot\n",
    "        return output_dup\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred_edited = {classname: 0 for classname in classes}\n",
    "    total_pred_edited = {classname: 0 for classname in classes}\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "            # get edited scores\n",
    "            with Trace(net, 'conv2', edit_output=remove_directions) as tr:\n",
    "                outputs_ = net(images)\n",
    "            _, predictions_ = torch.max(outputs_, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions_):\n",
    "                if label == prediction:\n",
    "                    correct_pred_edited[classes[label]] += 1\n",
    "                total_pred_edited[classes[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    accs = []\n",
    "    cls = []\n",
    "    for original, edited in zip(correct_pred.items(),correct_pred_edited.items()):\n",
    "        accuracy_original = 100 * float(original[1]) / total_pred[original[0]]\n",
    "        accuracy_edited = 100 * float(edited[1]) / total_pred[edited[0]]\n",
    "        perc_changed = (accuracy_original - accuracy_edited)\n",
    "        accs.append(f'{perc_changed:.1f}')\n",
    "        cls.append(original[0])\n",
    "#         print(f'Accuracy for class: {original[0]:5s} \\t Original {accuracy_original:.1f} % \\t Edited {accuracy_edited:.1f} % \\t Change {perc_changed:.1f} %')\n",
    "    text = f'{removing_object} '\n",
    "    for a in accs:\n",
    "        text+=f'& {a} '\n",
    "    print(text + ' \\\\\\\\')\n",
    "#     print(cls)\n",
    "        \n",
    "    del images_class, images_nonclass, labels_class, labels_nonclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5362f1c9",
   "metadata": {},
   "source": [
    "# Top 1 Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84ce56e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane & 4.3 & -1.7 & 6.1 & 2.1 & 5.5 & 23.2 & 26.2 & 18.0 & 0.0 & 0.1  \\\\\n",
      "car & 9.3 & 0.3 & -8.0 & 10.8 & 4.9 & 15.7 & 23.1 & 7.1 & 0.0 & -3.1  \\\\\n",
      "bird & 18.8 & -3.4 & -1.7 & -1.4 & 3.8 & 11.2 & 15.9 & 3.0 & 14.5 & -5.6  \\\\\n",
      "cat & 36.7 & 1.8 & 1.6 & -2.5 & -0.6 & 3.9 & 7.3 & -3.4 & 4.3 & 2.9  \\\\\n",
      "deer & 27.8 & 2.8 & 1.8 & -0.4 & 3.0 & 4.9 & 18.5 & -1.8 & 16.9 & -2.4  \\\\\n",
      "dog & -0.5 & -0.5 & -2.7 & -4.0 & 5.5 & 7.4 & 2.0 & 1.5 & -0.1 & 4.5  \\\\\n",
      "frog & 22.1 & 2.1 & -1.2 & -1.8 & 4.1 & -0.8 & 2.4 & -0.8 & -4.7 & 0.9  \\\\\n",
      "horse & -2.0 & -1.0 & 0.3 & -1.4 & 0.2 & 0.1 & -1.3 & 2.9 & 5.5 & -2.8  \\\\\n",
      "ship & 3.2 & -2.3 & 10.6 & 14.8 & 8.9 & 29.4 & 29.3 & 18.7 & 1.4 & -0.7  \\\\\n",
      "truck & 7.8 & 2.2 & -5.4 & 11.6 & 3.7 & 18.1 & 24.3 & 7.7 & 0.7 & -4.6  \\\\\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    eval_top_eigens(class_idx = i, top_eigens = 1,trainloader=trainloader,testloader=testloader)\n",
    "#     print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09fe81c",
   "metadata": {},
   "source": [
    "# Top 3 Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "712f3a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane & 6.8 & 8.1 & 24.6 & 25.8 & 29.0 & 33.9 & 31.4 & 24.0 & -0.9 & -2.3  \\\\\n",
      "car & 16.7 & 7.4 & 18.1 & 14.6 & 5.3 & 23.5 & 32.1 & 15.5 & 2.9 & -6.6  \\\\\n",
      "bird & 24.7 & 17.8 & 10.1 & -3.3 & 3.4 & 15.3 & 25.1 & 1.3 & 13.3 & -7.5  \\\\\n",
      "cat & 41.5 & 14.3 & 14.5 & -11.0 & 5.5 & 8.7 & 12.3 & -2.6 & 35.5 & 1.4  \\\\\n",
      "deer & 30.3 & 10.9 & 3.2 & 2.2 & 1.6 & 14.9 & 17.9 & 1.4 & 28.5 & -7.0  \\\\\n",
      "dog & 17.0 & 0.2 & 12.1 & -6.7 & 2.0 & 16.0 & 8.2 & 1.0 & 22.4 & -4.6  \\\\\n",
      "frog & 36.3 & 35.1 & 10.2 & 2.4 & 6.6 & 6.4 & 10.0 & -1.1 & 15.1 & -7.4  \\\\\n",
      "horse & 12.7 & 5.2 & 7.8 & 3.7 & -0.5 & 6.5 & 21.0 & 7.4 & 8.8 & -4.8  \\\\\n",
      "ship & -1.6 & 3.6 & 23.2 & 25.2 & 48.3 & 31.7 & 32.0 & 30.6 & 7.1 & -4.2  \\\\\n",
      "truck & 15.3 & 8.5 & 16.0 & 17.7 & 24.6 & 23.4 & 31.4 & 12.9 & -1.1 & -2.2  \\\\\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    eval_top_eigens(class_idx = i, top_eigens = 5,trainloader=trainloader,testloader=testloader)\n",
    "#     print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8563745",
   "metadata": {},
   "source": [
    "# Top 5 Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "852c9362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane & 11.0 & 4.4 & 20.3 & 21.0 & 29.5 & 28.3 & 32.1 & 27.2 & -3.2 & -1.3  \\\\\n",
      "---------------------------------------------------------------\n",
      "car & 11.5 & 6.4 & 11.4 & 16.2 & 18.6 & 21.4 & 31.8 & 17.8 & -1.0 & -1.5  \\\\\n",
      "---------------------------------------------------------------\n",
      "bird & 25.2 & 10.5 & 4.8 & -3.7 & 2.4 & 14.5 & 23.7 & 1.9 & 9.0 & -5.7  \\\\\n",
      "---------------------------------------------------------------\n",
      "cat & 17.6 & -0.5 & -0.9 & -2.1 & -1.3 & 12.9 & 19.6 & 2.8 & 16.4 & -6.7  \\\\\n",
      "---------------------------------------------------------------\n",
      "deer & 30.8 & 7.6 & 4.2 & 5.2 & 3.1 & 18.1 & 21.1 & 0.1 & 7.5 & -1.9  \\\\\n",
      "---------------------------------------------------------------\n",
      "dog & 34.1 & 3.7 & 11.5 & -9.7 & 5.6 & 1.3 & 10.0 & -2.3 & 28.1 & 4.2  \\\\\n",
      "---------------------------------------------------------------\n",
      "frog & 51.9 & 44.2 & 4.2 & -6.8 & -2.8 & 0.7 & -3.1 & -2.6 & 48.1 & 16.7  \\\\\n",
      "---------------------------------------------------------------\n",
      "horse & 21.6 & -2.4 & 2.9 & 2.5 & 0.6 & 8.7 & 16.2 & 1.5 & 12.3 & -2.7  \\\\\n",
      "---------------------------------------------------------------\n",
      "ship & 0.2 & -0.9 & 25.2 & 31.6 & 50.6 & 36.1 & 32.5 & 45.1 & -5.5 & 12.2  \\\\\n",
      "---------------------------------------------------------------\n",
      "truck & 10.2 & 0.7 & 7.6 & 21.0 & 11.6 & 24.9 & 30.9 & 5.2 & 5.1 & -4.2  \\\\\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    eval_top_eigens(class_idx = i, top_eigens = 3,trainloader=trainloader,testloader=testloader)\n",
    "    print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5c141",
   "metadata": {},
   "source": [
    "# Consolidated function to edit FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ae79af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e5beb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar, TraceDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "# #         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# net = Net()\n",
    "# PATH = './cifar_net.pth'\n",
    "# net = Net()\n",
    "# net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f78594",
   "metadata": {},
   "source": [
    "# Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48f9f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_class(class_,trainloader):\n",
    "    images_class = []\n",
    "    labels_class = []\n",
    "    images_nonclass = []\n",
    "    labels_nonclass = []\n",
    "\n",
    "    for data in trainloader:\n",
    "        if len(images_class) == 250 and len(images_nonclass)==250:\n",
    "            break\n",
    "        images, labels = data\n",
    "        for label, image in zip(labels,images):\n",
    "            if label == class_:\n",
    "                images_class.append(image)\n",
    "                labels_class.append(label)\n",
    "            else:\n",
    "                if len(images_nonclass)<250:\n",
    "                    images_nonclass.append(image)\n",
    "                    labels_nonclass.append(label)\n",
    "    try:\n",
    "        images_class = torch.stack(images_class)\n",
    "        labels_class = torch.stack(labels_class)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    images_nonclass = torch.stack(images_nonclass)\n",
    "    labels_nonclass = torch.stack(labels_nonclass)\n",
    "    del(data)\n",
    "    return images_class, images_nonclass, labels_class, labels_nonclass\n",
    "\n",
    "def eval_top_eigens(class_idx, top_eigens, trainloader, testloader,layer = 'fc1'):\n",
    "    images_class, images_nonclass, labels_class, labels_nonclass = get_data_class(class_idx, trainloader)\n",
    "    view_output = []\n",
    "    def hook_fn(module, input, output):\n",
    "        view_output.append(output)\n",
    "\n",
    "    ## Place hooks\n",
    "    view_output = []\n",
    "    def hook_fn(module, input, output):\n",
    "        view_output.append(output)\n",
    "    view_output = []\n",
    "    if layer == 'fc2':\n",
    "        hook = net.fc2.register_forward_hook(hook_fn)\n",
    "    elif layer == 'fc1':\n",
    "        hook = net.fc1.register_forward_hook(hook_fn)\n",
    "    else:\n",
    "        hook = net.fc3.register_forward_hook(hook_fn)\n",
    "    out = net(images_nonclass.to(device))\n",
    "    hook.remove()\n",
    "    torchtensor_nonclass = view_output[0]\n",
    "    \n",
    "    view_output = [] \n",
    "    if layer == 'fc2':\n",
    "        hook = net.fc2.register_forward_hook(hook_fn)\n",
    "    elif layer == 'fc1':\n",
    "        hook = net.fc1.register_forward_hook(hook_fn)\n",
    "    else:\n",
    "        hook = net.classifier[6].register_forward_hook(hook_fn)\n",
    "    out = net(images_class.to(device))\n",
    "    hook.remove()\n",
    "    # SVD\n",
    "    view_output[0] = view_output[0] - view_output[0].mean(0)\n",
    "    torchtensor = view_output[0]\n",
    "    mean_direction = torch.cat([torchtensor_nonclass, torchtensor]).mean(0)\n",
    "    u,s,v = torch.svd(torchtensor.T)\n",
    "    important_direction = u.T[:top_eigens]\n",
    "#     print(f'Removing Eigen Directions of class: {classes[class_idx]}')\n",
    "    removing_object = classes[class_idx]\n",
    "    ## Choose important eigen vector and remove the directions\n",
    "    from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar, TraceDict\n",
    "    views = []\n",
    "    def remove_directions(output,layer):\n",
    "        if len(important_direction) == 0:\n",
    "            return output\n",
    "        output_dup = output.detach().clone()\n",
    "        for i in range(top_eigens):\n",
    "            for j in range(output.shape[0]):\n",
    "                magnitude = torch.norm( important_direction[i])\n",
    "                mean_dot = torch.dot(mean_direction, important_direction[i])/magnitude\n",
    "                dot = torch.dot(output[j], important_direction[i])/magnitude\n",
    "                output_dup[j] = output_dup[j] - important_direction[i]*dot + important_direction[i]*mean_dot\n",
    "        views.append(output_dup)\n",
    "        return output_dup\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred_edited = {classname: 0 for classname in classes}\n",
    "    total_pred_edited = {classname: 0 for classname in classes}\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images.to(device))\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "            # get edited scores\n",
    "            with Trace(net, layer, edit_output=remove_directions) as tr:\n",
    "                outputs_ = net(images.to(device))\n",
    "            _, predictions_ = torch.max(outputs_, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions_):\n",
    "                if label == prediction:\n",
    "                    correct_pred_edited[classes[label]] += 1\n",
    "                total_pred_edited[classes[label]] += 1\n",
    "\n",
    "    accs = []\n",
    "    cls = []\n",
    "    for original, edited in zip(correct_pred.items(),correct_pred_edited.items()):\n",
    "        accuracy_original = 100 * float(original[1]) / total_pred[original[0]]\n",
    "        accuracy_edited = 100 * float(edited[1]) / total_pred[edited[0]]\n",
    "        perc_changed = (accuracy_original - accuracy_edited)\n",
    "        accs.append(f'{perc_changed:.1f}')\n",
    "        cls.append(original[0])\n",
    "#         print(f'Accuracy for class: {original[0]:5s} \\t Original {accuracy_original:.1f} % \\t Edited {accuracy_edited:.1f} % \\t Change {perc_changed:.1f} %')\n",
    "    text = f'{removing_object} '\n",
    "    for a in accs:\n",
    "        text+=f'& {a} '\n",
    "    print(text + ' \\\\\\\\')\n",
    "#     # print accuracy for each class\n",
    "#     for original, edited in zip(correct_pred.items(),correct_pred_edited.items()):\n",
    "#         accuracy_original = 100 * float(original[1]) / total_pred[original[0]]\n",
    "#         accuracy_edited = 100 * float(edited[1]) / total_pred[edited[0]]\n",
    "#         perc_changed = (accuracy_original - accuracy_edited)\n",
    "#         print(f'Accuracy for class: {original[0]:5s} \\t Original {accuracy_original:.1f} % \\t Edited {accuracy_edited:.1f} % \\t Change {perc_changed:.1f} %')\n",
    "    del images_class, images_nonclass, labels_class, labels_nonclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d454fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_class(class_,trainloader):\n",
    "    images_class = []\n",
    "    labels_class = []\n",
    "    images_nonclass = []\n",
    "    labels_nonclass = []\n",
    "\n",
    "    for data in trainloader:\n",
    "        if len(images_class) == 250 and len(images_nonclass)==250:\n",
    "            break\n",
    "        images, labels = data\n",
    "        for label, image in zip(labels,images):\n",
    "            if label == class_:\n",
    "                images_class.append(image)\n",
    "                labels_class.append(label)\n",
    "            else:\n",
    "                if len(images_nonclass)<250:\n",
    "                    images_nonclass.append(image)\n",
    "                    labels_nonclass.append(label)\n",
    "    try:\n",
    "        images_class = torch.stack(images_class)\n",
    "        labels_class = torch.stack(labels_class)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    images_nonclass = torch.stack(images_nonclass)\n",
    "    labels_nonclass = torch.stack(labels_nonclass)\n",
    "    del(data)\n",
    "    return images_class, images_nonclass, labels_class, labels_nonclass\n",
    "\n",
    "def eval_top_eigens(class_idx, top_eigens, trainloader, testloader,layer = 'fc1'):\n",
    "    images_class, images_nonclass, labels_class, labels_nonclass = get_data_class(class_idx, trainloader)\n",
    "    view_output = []\n",
    "    def hook_fn(module, input, output):\n",
    "        view_output.append(output)\n",
    "\n",
    "    ## Place hooks\n",
    "    view_output = []\n",
    "    def hook_fn(module, input, output):\n",
    "        view_output.append(output)\n",
    "    view_output = []\n",
    "    if layer == 'fc2':\n",
    "        hook = net.fc2.register_forward_hook(hook_fn)\n",
    "    elif layer == 'fc1':\n",
    "        hook = net.fc1.register_forward_hook(hook_fn)\n",
    "    else:\n",
    "        hook = net.classifier[6].register_forward_hook(hook_fn)\n",
    "    out = net(images_class.to(device))\n",
    "    hook.remove()\n",
    "    # SVD\n",
    "    view_output[0] = view_output[0] - view_output[0].mean(0)\n",
    "    torchtensor = view_output[0]\n",
    "    u,s,v = torch.svd(torchtensor.T)\n",
    "    important_direction = u.T[:top_eigens]\n",
    "#     print(f'Removing Eigen Directions of class: {classes[class_idx]}')\n",
    "    ## Choose important eigen vector and remove the directions\n",
    "    removing_object = classes[class_idx]\n",
    "    from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar, TraceDict\n",
    "    views = []\n",
    "    def remove_directions(output,layer):\n",
    "        if len(important_direction) == 0:\n",
    "            return output\n",
    "        output_dup = output.detach().clone()\n",
    "        for i in range(top_eigens):\n",
    "            for j in range(output.shape[0]):\n",
    "                dot = torch.dot(output[j], important_direction[i])\n",
    "                output_dup[j] = output_dup[j] - important_direction[i]*dot\n",
    "        views.append(output_dup)\n",
    "        return output_dup\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred_edited = {classname: 0 for classname in classes}\n",
    "    total_pred_edited = {classname: 0 for classname in classes}\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images.to(device))\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "            # get edited scores\n",
    "            with Trace(net, layer, edit_output=remove_directions) as tr:\n",
    "                outputs_ = net(images.to(device))\n",
    "            _, predictions_ = torch.max(outputs_, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions_):\n",
    "                if label == prediction:\n",
    "                    correct_pred_edited[classes[label]] += 1\n",
    "                total_pred_edited[classes[label]] += 1\n",
    "\n",
    "    accs = []\n",
    "    cls = []\n",
    "    for original, edited in zip(correct_pred.items(),correct_pred_edited.items()):\n",
    "        accuracy_original = 100 * float(original[1]) / total_pred[original[0]]\n",
    "        accuracy_edited = 100 * float(edited[1]) / total_pred[edited[0]]\n",
    "        perc_changed = (accuracy_original - accuracy_edited)\n",
    "        accs.append(f'{perc_changed:.1f}')\n",
    "        cls.append(original[0])\n",
    "#         print(f'Accuracy for class: {original[0]:5s} \\t Original {accuracy_original:.1f} % \\t Edited {accuracy_edited:.1f} % \\t Change {perc_changed:.1f} %')\n",
    "    text = f'{removing_object} '\n",
    "    for a in accs:\n",
    "        text+=f'& {a} '\n",
    "    print(text + ' \\\\\\\\')\n",
    "#     # print accuracy for each class\n",
    "#     for original, edited in zip(correct_pred.items(),correct_pred_edited.items()):\n",
    "#         accuracy_original = 100 * float(original[1]) / total_pred[original[0]]\n",
    "#         accuracy_edited = 100 * float(edited[1]) / total_pred[edited[0]]\n",
    "#         perc_changed = (accuracy_original - accuracy_edited)\n",
    "#         print(f'Accuracy for class: {original[0]:5s} \\t Original {accuracy_original:.1f} % \\t Edited {accuracy_edited:.1f} % \\t Change {perc_changed:.1f} %')\n",
    "    del images_class, images_nonclass, labels_class, labels_nonclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32adc7",
   "metadata": {},
   "source": [
    "# Top 1 Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "408550a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane & -2.1 & 2.5 & -0.9 & -3.6 & 7.6 & 4.0 & -4.7 & -3.1 & -3.2 & 18.9  \\\\\n",
      "car & -2.8 & 91.4 & -0.6 & 10.6 & 9.2 & -3.9 & -4.2 & -1.5 & -1.5 & -1.7  \\\\\n",
      "bird & -2.1 & -0.1 & 2.3 & -4.3 & 1.7 & 6.1 & -3.7 & -2.4 & -1.8 & 13.9  \\\\\n",
      "cat & -2.0 & 91.0 & -2.5 & 16.0 & 1.8 & -0.3 & -3.9 & -2.7 & -2.1 & 1.7  \\\\\n",
      "deer & -2.5 & 2.2 & -2.5 & -3.7 & 46.7 & 0.5 & -3.8 & -2.2 & -3.5 & 6.0  \\\\\n",
      "dog & 2.0 & -2.7 & -2.2 & -7.4 & -0.7 & 77.1 & -2.4 & -3.2 & -2.7 & 21.5  \\\\\n",
      "frog & -0.2 & 0.1 & -1.4 & -3.1 & 4.8 & 4.3 & -3.0 & -3.2 & -2.7 & 25.1  \\\\\n",
      "horse & -1.1 & -0.3 & -1.7 & -1.7 & -4.2 & -0.7 & 1.9 & 89.4 & 10.7 & -1.7  \\\\\n",
      "ship & -2.1 & -0.6 & -0.3 & -0.1 & -2.1 & -2.2 & -0.2 & 13.6 & 85.2 & -1.6  \\\\\n",
      "truck & -3.0 & -0.9 & -0.1 & -5.6 & 7.1 & 5.1 & -3.5 & -3.3 & -3.4 & 86.6  \\\\\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    eval_top_eigens(class_idx = i, top_eigens = 1,trainloader=trainloader,testloader=testloader, layer='classifier.6')\n",
    "#     print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79baf8ed",
   "metadata": {},
   "source": [
    "# Top 3 Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff34b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane & 69.1 & 91.2 & 0.7 & -16.7 & 59.7 & 79.9 & -5.9 & 88.3 & 68.0 & 87.5  \\\\\n",
      "---------------------------------------------------------------\n",
      "car & -8.4 & 91.6 & -9.5 & 72.2 & -6.7 & 79.9 & 80.3 & 88.6 & 71.9 & 87.3  \\\\\n",
      "---------------------------------------------------------------\n",
      "bird & 73.3 & 91.3 & 78.6 & -8.7 & 17.5 & 80.3 & -7.9 & 89.5 & -4.5 & 87.7  \\\\\n",
      "---------------------------------------------------------------\n",
      "cat & -3.6 & 91.5 & -10.8 & 72.5 & -6.8 & 80.2 & 79.4 & 89.5 & 45.7 & 71.5  \\\\\n",
      "---------------------------------------------------------------\n",
      "deer & -0.9 & 91.5 & 0.6 & 73.1 & 85.0 & 80.2 & -11.0 & 89.4 & -4.8 & 87.7  \\\\\n",
      "---------------------------------------------------------------\n",
      "dog & -8.7 & 91.5 & -5.6 & 73.5 & 81.6 & 80.0 & -8.3 & 87.7 & 85.2 & 26.3  \\\\\n",
      "---------------------------------------------------------------\n",
      "frog & 0.1 & 91.3 & -11.2 & 61.9 & -6.1 & 79.9 & 79.8 & 89.3 & 84.4 & 87.9  \\\\\n",
      "---------------------------------------------------------------\n",
      "horse & 0.4 & 91.4 & -1.6 & 72.7 & 83.9 & 80.0 & -10.3 & 89.5 & -4.8 & 87.7  \\\\\n",
      "---------------------------------------------------------------\n",
      "ship & -4.5 & 91.2 & -11.8 & 72.7 & -5.4 & 79.5 & 79.4 & 89.0 & 84.1 & 87.2  \\\\\n",
      "---------------------------------------------------------------\n",
      "truck & 15.0 & 91.1 & -6.1 & -6.9 & -6.4 & 79.7 & 79.5 & 89.3 & 84.8 & 87.9  \\\\\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    eval_top_eigens(class_idx = i, top_eigens = 5,trainloader=trainloader,testloader=testloader, layer='classifier.6')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5d3c8",
   "metadata": {},
   "source": [
    "# Top 5 Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e4539c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane & 35.3 & 38.0 & -4.7 & -4.8 & 4.5 & 4.9 & -4.4 & 2.5 & -1.7 & 28.9  \\\\\n",
      "car & -6.1 & 91.3 & -2.9 & 0.7 & 1.1 & 7.5 & -7.4 & 9.3 & 84.5 & 86.3  \\\\\n",
      "bird & 0.6 & -0.4 & 26.0 & -9.6 & -2.4 & 22.2 & -4.0 & 89.6 & 3.2 & 25.1  \\\\\n",
      "cat & -5.8 & 90.6 & -4.5 & 72.5 & -4.2 & -2.9 & -6.6 & 89.9 & 2.0 & 50.7  \\\\\n",
      "deer & 6.7 & 5.7 & -7.5 & -7.7 & 75.1 & 80.1 & -4.7 & 89.7 & -3.4 & 3.7  \\\\\n",
      "dog & -4.6 & 91.4 & -3.7 & -12.3 & 1.7 & 79.5 & -5.6 & 89.7 & 2.6 & 71.9  \\\\\n",
      "frog & -0.2 & 91.2 & -5.3 & -8.6 & 2.1 & 60.5 & 71.5 & 11.2 & -3.9 & 4.5  \\\\\n",
      "horse & -3.2 & 91.0 & -6.7 & -12.9 & 14.5 & 79.8 & -5.3 & 89.4 & 2.8 & 15.5  \\\\\n",
      "ship & 4.3 & 90.1 & -4.9 & -6.7 & 1.3 & -0.4 & -1.8 & 26.3 & 84.7 & 16.4  \\\\\n",
      "truck & -6.2 & 91.4 & -2.3 & -8.1 & 0.0 & 73.5 & -6.6 & 25.2 & 48.2 & 86.0  \\\\\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    eval_top_eigens(class_idx = i, top_eigens = 3,trainloader=trainloader,testloader=testloader, layer='classifier.6')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dda1a5",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32db81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing the layer outputs\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "images_class, images_nonclass, labels_class, labels_nonclass = get_data_class(100)\n",
    "view_output = []\n",
    "def hook_fn(module, input, output):\n",
    "    view_output.append(output)\n",
    "\n",
    "# again no gradients needed\n",
    "view_output = []\n",
    "hook = net.conv2.register_forward_hook(hook_fn)\n",
    "with torch.no_grad():\n",
    "    outputs = net(images_nonclass)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "hook.remove()\n",
    "\n",
    "out = view_output[0].flatten(1,-1)\n",
    "\n",
    "df = pd.DataFrame(out.detach().numpy())\n",
    "\n",
    "data_subset = df.values\n",
    "\n",
    "df['y'] = labels_nonclass.detach().numpy()\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_subset)\n",
    "\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "\n",
    "df['tsne-2d-one'] = tsne_results[:,0]\n",
    "df['tsne-2d-two'] = tsne_results[:,1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.color_palette(\"tab10\")\n",
    "sns.lmplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    fit_reg=False,\n",
    "    hue=\"y\",\n",
    "#     palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "#     alpha=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ed326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "plot.figure(figsize = (5, 4))\n",
    "plot_axes = plot.axes(projection = '3d')\n",
    "print(type(plot_axes))\n",
    "plot_axes.scatter3D(df[\"tsne-2d-one\"], df[\"tsne-2d-two\"],df['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a602d",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97fb65",
   "metadata": {},
   "source": [
    "The final layer acts like a logistic regression, therefore, removing important directions could have some interpretable effect on the performance. If we remove direction of 1 class, there is effect on that class. But not for all other classes. Similarly, we find that it is not possible to erase a direction from early layers and draw meaningful conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
